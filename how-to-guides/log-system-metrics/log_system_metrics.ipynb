{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch neptune_scale GPUtil psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1742913015936,
     "user": {
      "displayName": "Leo Breedt",
      "userId": "01966169980594931892"
     },
     "user_tz": -60
    },
    "id": "_FjB34cOh480"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "import queue\n",
    "import GPUtil\n",
    "import psutil\n",
    "from neptune_scale import Run\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742913017736,
     "user": {
      "displayName": "Leo Breedt",
      "userId": "01966169980594931892"
     },
     "user_tz": -60
    },
    "id": "9aTCZUXjh481",
    "outputId": "127d4cb6-cbf7-4e68-b1dc-8bec73364fc1"
   },
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = \"YOUR_API_TOKEN\"\n",
    "# %env NEPTUNE_PROJECT = \"WORKSPACE/PROJECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1742913316971,
     "user": {
      "displayName": "Leo Breedt",
      "userId": "01966169980594931892"
     },
     "user_tz": -60
    },
    "id": "I1QEeWESh481",
    "outputId": "1030397c-d352-4e0d-84c5-770164e6b9f5"
   },
   "outputs": [],
   "source": [
    "# Initialize Neptune Run object\n",
    "# Set parameters\n",
    "params = {\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"num_gpus\": torch.cuda.device_count(),\n",
    "}\n",
    "\n",
    "run = Run(\n",
    "    run_id=f\"system-{uuid4()}\"\n",
    ")\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        # \"config/learning_rate\": params[\"device\"],\n",
    "        \"config/optimizer\": params[\"num_gpus\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[\"GPU\"], group_tags=True)\n",
    "run.add_tags(tags=[\"monitor_gpu\", \"single-node\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to track system (CPU and GPU) usage and log to Neptune\n",
    "## This function will be launched on a background thread to track the metrics at each step\n",
    "def track_system_metrics(run, event):\n",
    "    monitoring_metrics = {}\n",
    "    step_counter = 0\n",
    "\n",
    "    while True:\n",
    "        event.wait()  # Wait until the training step signals readiness\n",
    "        step_counter += 1\n",
    "\n",
    "        # Get cpu usage\n",
    "        monitoring_metrics[\"system/monitor/CPU/percent\"] = psutil.cpu_percent()\n",
    "\n",
    "        # Get gpu usage\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            gpu_id = gpu.id\n",
    "            gpu_name = gpu.name\n",
    "\n",
    "            monitoring_metrics[\n",
    "                f\"system/monitor/GPU/{gpu_name}-{gpu_id}-{gpu.uuid}/memory_used_GB\"\n",
    "            ] = (gpu.memoryUsed / 1024)\n",
    "            monitoring_metrics[\n",
    "                f\"system/monitor/GPU/{gpu_name}-{gpu_id}-{gpu.uuid}/memory_total_GB\"\n",
    "            ] = (gpu.memoryTotal / 1024)\n",
    "            monitoring_metrics[\n",
    "                f\"system/monitor/GPU/{gpu_name}-{gpu_id}-{gpu.uuid}/memory_utilized_percent\"\n",
    "            ] = (\n",
    "                gpu.memoryUtil * 100\n",
    "            )  # Percentage\n",
    "            monitoring_metrics[\n",
    "                f\"system/monitor/GPU/{gpu_name}-{gpu_id}-{gpu.uuid}/memory_free_GB\"\n",
    "            ] = (\n",
    "                gpu.memoryFree / 1024\n",
    "            )  # in MB\n",
    "            monitoring_metrics[\n",
    "                f\"system/monitor/GPU/{gpu_name}-{gpu_id}-{gpu.uuid}/temperature_celsius\"\n",
    "            ] = gpu.temperature  # Celsius\n",
    "\n",
    "        # Log system metrics to Neptune\n",
    "        run.log_metrics(data=monitoring_metrics, step=step_counter)\n",
    "\n",
    "        event.clear()  # Reset event to wait for next training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165522,
     "status": "ok",
     "timestamp": 1742913207429,
     "user": {
      "displayName": "Leo Breedt",
      "userId": "01966169980594931892"
     },
     "user_tz": -60
    },
    "id": "TbXC3Pzsh482",
    "outputId": "fceed1ff-9a1c-4069-eb06-5dc0e766cb6b"
   },
   "outputs": [],
   "source": [
    "# Dummy dataset and model for demonstration\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, size=1000):\n",
    "        self.data = torch.randn(size, 10)\n",
    "        self.target = torch.randint(0, 2, (size,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleModel()\n",
    "\n",
    "# Use DataParallel for multi-GPU setup\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dataset = SimpleDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Queue to store GPU metrics from the background thread\n",
    "# metrics_queue = queue.Queue()\n",
    "\n",
    "# Event for synchronizing the training loop with the GPU metrics collection\n",
    "event = threading.Event()\n",
    "\n",
    "# Start the background thread to track GPU metrics\n",
    "system_monitoring_thread = threading.Thread(\n",
    "    target=track_system_metrics, args=(run, event), daemon=True\n",
    ")\n",
    "system_monitoring_thread.start()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "step_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        time.sleep(0.5)\n",
    "        step_counter += 1\n",
    "        start_time = time.time()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Signal the system monitoring thread to log metrics after every step after the backward pass is complete\n",
    "        # metrics_queue.put(step_counter)\n",
    "        event.set()\n",
    "\n",
    "        # Track running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} complete.\")\n",
    "\n",
    "print(\"Training Finished!\")\n",
    "\n",
    "run.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
