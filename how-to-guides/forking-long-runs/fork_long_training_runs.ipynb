{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fork Long Training Runs with Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lb/forking-long-runs/how-to-guides/forking-long-runs/fork_long_training_runs.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/neptune-ai/scale-examples/blob/lb/forking-long-runs/how-to-guides/debug-model-training-runs/debug_training_runs.ipynb\">\n",
    "  <img alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open_in_GitHub-blue?logo=github&labelColor=black\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://docs-beta.neptune.ai/tutorials/\">\n",
    "  <img alt=\"View tutorial in docs\" src=\"https://neptune.ai/wp-content/uploads/2024/01/docs-badge-2.svg\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Training large models can take weeks or months. During these long runs, you may need to:\n",
    "- Resume from checkpoints after interruptions \n",
    "- Fork runs to try different hyperparameters\n",
    "- Monitor total training progress and lineage\n",
    "\n",
    "This tutorial shows you how to:\n",
    "1. **Initialize Neptune** for long runs\n",
    "2. Create a **forked training run** when needed\n",
    "3. **Restart training** from checkpoints\n",
    "\n",
    "Step through a pre-configured report:\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/leo/pytorch-tutorial/reports/9ec24024-08dd-45c4-8d82-51b916054fb6\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "\n",
    "_Note: This is a code recipe that you can adapt for your own model training needs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "Set your project name and API token as environment variables to log to your Neptune Scale project.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = YOUR_API_TOKEN\n",
    "# %env NEPTUNE_PROJECT = WORKSPACE_NAME/PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training simulation\n",
    "\n",
    "This tutorial uses a simulated training scenario to demonstrate Neptune's forking capabilities. The simulation generates synthetic training metrics that mimic real model training, including:\n",
    "- Gradually decreasing loss\n",
    "- Increasing accuracy \n",
    "- Random noise and occasional spikes\n",
    "\n",
    "In the next cell, we'll:\n",
    "1. Import required libraries (numpy, json, etc.)\n",
    "2. Create a `TrainingMetrics` class that simulates training progression\n",
    "3. Define utility functions for saving model state (`save_checkpoint()` and `load_checkpoint()`)\n",
    "\n",
    "_You can replace this simulation with your actual model training code._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "import math\n",
    "\n",
    "\n",
    "class TrainingMetrics:\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_loss: float = 1.0,\n",
    "        initial_accuracy: float = 0.0,\n",
    "        noise_scale: float = 0.1,\n",
    "        loss_trend: float = -0.1,\n",
    "        accuracy_trend: float = 0.1,\n",
    "    ):\n",
    "        self.previous_loss = initial_loss\n",
    "        self.previous_accuracy = initial_accuracy\n",
    "        self.noise_scale = noise_scale\n",
    "        self.loss_trend = loss_trend\n",
    "        self.accuracy_trend = accuracy_trend\n",
    "        self.step_count = 1\n",
    "\n",
    "        # Random convergence points\n",
    "        self.target_loss = np.random.uniform(0.0, 1.0)\n",
    "        self.target_accuracy = np.random.uniform(0.0, 1.0)\n",
    "\n",
    "    def update_metrics(self, spikes=True) -> tuple[float, float]:\n",
    "        \"\"\"Update loss and accuracy using a random walk process with logarithmic trends\"\"\"\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Base loss update with normal progression\n",
    "        decay_factor = math.log(1 + abs(self.previous_loss - self.target_loss))\n",
    "        loss_step = self.noise_scale * np.random.randn() + self.loss_trend * decay_factor\n",
    "        loss_step *= 1 + 0.1 * math.log(self.step_count)\n",
    "\n",
    "        # Check for spike/anomaly (0.01% chance) after step 10k\n",
    "        if spikes and np.random.random() < 0.0001 and self.step_count > 10000:\n",
    "            # Generate a sudden spike, independent of current loss\n",
    "            spike_magnitude = np.random.uniform(1, 10)  # Random spike between 1x and 5x\n",
    "            current_loss = spike_magnitude\n",
    "        else:\n",
    "            # Normal progression\n",
    "            current_loss = max(0.0, self.previous_loss + loss_step)\n",
    "            self.previous_loss = current_loss\n",
    "\n",
    "        # self.previous_loss = current_loss\n",
    "        current_accuracy = 1 - current_loss\n",
    "\n",
    "        return current_loss, current_accuracy\n",
    "\n",
    "\n",
    "def save_checkpoint(metrics: TrainingMetrics, epoch: int, loss: float, accuracy: float) -> None:\n",
    "    \"\"\"Save training state to disk\"\"\"\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"step_count\": metrics.step_count,\n",
    "        \"noise_scale\": metrics.noise_scale,\n",
    "        \"loss_trend\": metrics.loss_trend,\n",
    "        \"accuracy_trend\": metrics.accuracy_trend,\n",
    "        \"target_loss\": metrics.target_loss,\n",
    "        \"target_accuracy\": metrics.target_accuracy,\n",
    "    }\n",
    "    with open(f\"checkpoint_epoch_{epoch}.json\", \"w\") as f:\n",
    "        json.dump(checkpoint, f)\n",
    "\n",
    "    print(f\"Checkpoint saved; epoch - {epoch}, step - {metrics.step_count}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(epoch: int) -> Dict[str, Any]:\n",
    "    \"\"\"Load training state from disk\"\"\"\n",
    "    with open(f\"checkpoint_epoch_{epoch}.json\", \"r\") as f:\n",
    "        checkpoint = json.load(f)\n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"noise_scale\": np.random.uniform(0.0005, 0.002),\n",
    "    \"loss_trend\": -np.random.uniform(0, 0.0002),\n",
    "    \"accuracy_trend\": np.random.uniform(0, 0.0002),\n",
    "}\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = TrainingMetrics(\n",
    "    initial_loss=np.random.uniform(3, 7),\n",
    "    initial_accuracy=0,\n",
    "    noise_scale=params[\"noise_scale\"],\n",
    "    loss_trend=params[\"loss_trend\"],\n",
    "    accuracy_trend=params[\"accuracy_trend\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forking run steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: _Create initial base experiment_\n",
    "\n",
    "Create a base experiment that represents a long training process. This could be your best performing model from hyperparameter optimization that you want to train further.\n",
    "\n",
    "The training loop will:\n",
    "1. Log training metrics (`loss` and `accuracy`) at each step\n",
    "2. Save model state every 10 epochs\n",
    "\n",
    "By saving checkpoints (or model state) periodically, we can restart training from any saved state if needed. Neptune's forking capability allows us to create new runs that inherit the training history, making it easy to visualize the complete training progression across multiple runs.\n",
    "\n",
    "See our [Quickstart](https://docs.neptune.ai/quickstart) if you have not created a Neptune run before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune_scale import Run\n",
    "\n",
    "run = Run(\n",
    "    experiment_name=\"forking-long\",  # Create a run that is the head of an experiment\n",
    ")\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[\"forking\", \"base run\"])\n",
    "\n",
    "print(f\"See configuration parameters:\\n{run.get_experiment_url() + '&detailsTab=charts'}\")\n",
    "\n",
    "step_counter = 0\n",
    "# Training loop\n",
    "for epoch in range(1, params[\"epochs\"]):\n",
    "    # Process batches\n",
    "    for batch in range(0, 10000, params[\"batch_size\"]):\n",
    "        # Update metrics for this batch\n",
    "        batch_loss, batch_accuracy = metrics.update_metrics()\n",
    "\n",
    "        run.log_metrics(\n",
    "            data={\n",
    "                \"metrics/train/loss\": batch_loss,\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=step_counter,\n",
    "        )\n",
    "        step_counter += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        save_checkpoint(metrics, epoch, loss=batch_loss, accuracy=batch_accuracy)\n",
    "\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load model state and create forked run\n",
    "\n",
    "As training instabilities start occuring after 50 epochs, we may decide to restart our training from one of our saved model states. We can then pick a model state to restart training from and tell Neptune to inherit our training history from the previous experiment. \n",
    "\n",
    "To create a forked run, we need to do the following;\n",
    "1. Load our saved model state\n",
    "2. Create a forked run using the Neptune `Run` object\n",
    "3. Log metrics to the forked run\n",
    "\n",
    "#### _Step 2.1: Load model state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint that you want to fork from\n",
    "checkpoint = load_checkpoint(epoch=50)\n",
    "\n",
    "# Initialize metrics with checkpoint values\n",
    "metrics = TrainingMetrics(\n",
    "    initial_loss=checkpoint[\"loss\"],\n",
    "    initial_accuracy=checkpoint[\"accuracy\"],\n",
    "    noise_scale=checkpoint[\"noise_scale\"],\n",
    "    loss_trend=checkpoint[\"loss_trend\"],\n",
    "    accuracy_trend=checkpoint[\"accuracy_trend\"],\n",
    ")\n",
    "\n",
    "metrics.step_count = checkpoint[\"step_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Step 2.2: Create a forked run_\n",
    "\n",
    "To create a forked run, you need:\n",
    "- The run ID of the original run to fork from\n",
    "  - Can be accessed via `run._run_id` property\n",
    "  - Can be found in Neptune web app\n",
    "  - Can be fetched programmatically using `neptune-fetcher` package\n",
    "- The step number to fork from\n",
    "\n",
    "Here's an example of how to create a forked run:\n",
    "```python\n",
    "forked_run = Run(\n",
    "    ...\n",
    "    fork_run_id=\"ID_OF_RUN_TO_FORK\", # ID of the run to fork from\n",
    "    fork_step=1234                   # Step at which to fork the run\n",
    ")\n",
    "```\n",
    "\n",
    "See [fork an experiment](https://docs.neptune.ai/fork_experiment) for more details of forking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a forked run\n",
    "print(f\"Forking run with ID: {run._run_id}\")\n",
    "\n",
    "forked_run = Run(\n",
    "    experiment_name=\"forking-long\",  # Becomes new head of the experiment\n",
    "    fork_run_id=run._run_id,\n",
    "    fork_step=checkpoint[\"step_count\"],\n",
    ")\n",
    "\n",
    "# Log the forked configuration\n",
    "forked_run.log_configs(\n",
    "    {\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "forked_run.add_tags(tags=[\"fork\"])\n",
    "\n",
    "print(forked_run.get_run_url() + \"&detailsTab=charts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Step 2.3: Log forked run metrics_\n",
    "\n",
    "Log metrics as normal using the `log_metrics()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f\"Forking from epoch - {checkpoint[\"epoch\"]} and, step - {checkpoint[\"step_count\"]}\")\n",
    "\n",
    "step_counter = checkpoint[\"step_count\"]\n",
    "for epoch in range(checkpoint[\"epoch\"] + 1, params[\"epochs\"]):\n",
    "    for batch in range(0, 10000, params[\"batch_size\"]):\n",
    "        # Update metrics for this batch\n",
    "        batch_loss, batch_accuracy = metrics.update_metrics(spikes=False)\n",
    "\n",
    "        # Log batch metrics to Neptune\n",
    "        forked_run.log_metrics(\n",
    "            data={\n",
    "                \"metrics/train/loss\": batch_loss,\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"epoch\": epoch,\n",
    "            },\n",
    "            step=step_counter,\n",
    "        )\n",
    "        step_counter += 1\n",
    "\n",
    "forked_run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: _Analyzing forked experiments_\n",
    "\n",
    "When you fork a run in Neptune, the system automatically tracks key relationships in the `sys/forking` namespace:\n",
    "\n",
    "| Property | Description | Example |\n",
    "|----------|-------------|---------|\n",
    "| `depth` | Number of forks from original run | 1 |\n",
    "| `parent` | ID of immediate parent run | expansive-strategy-20250428124036156-fzp9y |\n",
    "| `step` | Training step where fork occurred | 9704 |\n",
    "\n",
    "#### Key Analysis Capabilities\n",
    "1. **Compare Runs**: Use Neptune's comparison mode to analyze base and forked runs side-by-side\n",
    "2. **Visualize Lineage**: Track relationships and evolution between forked runs\n",
    "3. **Iterate & Improve**: Create additional forks to explore different approaches\n",
    "\n",
    "For a practical example, check out this [training report](https://scale.neptune.ai/leo/pytorch-tutorial/reports/9ec24024-08dd-45c4-8d82-51b916054fb6) showcasing forking in action.\n",
    "\n",
    "#### Next Steps\n",
    "- **Experiment with Multiple Forks**: Create forks from different checkpoints to explore various training paths\n",
    "- **Parameter Exploration**: Test different hyperparameters and architectures in separate forks\n",
    "- **Comparative Analysis**: Leverage Neptune's UI and run tables for in-depth result comparison\n",
    "- **Collaboration**: Document your findings in detailed reports to share insights with your team\n",
    "- **Version Control**: Use forks as checkpoints to maintain a history of model improvements\n",
    "\n",
    "See also: [SPECIFIC EXAMPLE USING PYTORCH]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
