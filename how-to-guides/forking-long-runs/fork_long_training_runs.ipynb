{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fork Long Training Runs with Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lb/forking-long-runs/how-to-guides/debug-model-training-runs/debug_training_runs.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/neptune-ai/scale-examples/blob/lb/forking-long-runs/how-to-guides/debug-model-training-runs/debug_training_runs.ipynb\">\n",
    "  <img alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open_in_GitHub-blue?logo=github&labelColor=black\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://docs-beta.neptune.ai/tutorials/\">\n",
    "  <img alt=\"View tutorial in docs\" src=\"https://neptune.ai/wp-content/uploads/2024/01/docs-badge-2.svg\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Training large models often requires running experiments for weeks or even months. During these extended training periods, you may need to:\n",
    "- Fork your training runs to explore different hyperparameters or architectures\n",
    "- Resume training from checkpoints after interruptions\n",
    "- Monitor long-term training stability and convergence\n",
    "\n",
    "Neptune provides robust support for managing long-running training experiments, allowing you to:\n",
    "- Fork existing runs to explore variations without losing progress\n",
    "- Track metrics and artifacts across multiple months of training\n",
    "- Maintain experiment lineage and reproducibility\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "1. **Initialize Neptune** and **log configuration parameters**\n",
    "2. **Fork existing training runs** to explore variations\n",
    "3. **Resume training** from checkpoints\n",
    "4. **Monitor long-term training stability**\n",
    "\n",
    "Step through a pre-configured report:\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/leo/pytorch-tutorial/reports/9e79d952-272a-4a38-83e5-27df4dd225ec\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "\n",
    "_Note: This is a code recipe that you can adapt for your own model training needs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "Set your project name and API token as environment variables to log to your Neptune Scale project.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = YOUR_API_TOKEN\n",
    "# %env NEPTUNE_PROJECT = WORKSPACE_NAME/PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale torch datasets\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 512,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 3,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"embed_size\": 1000,\n",
    "    \"hidden_size\": 256,  # hidden size for the LSTM\n",
    "    \"dropout_prob\": 0.3,\n",
    "    \"num_lstm_layers\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data, model and other dependencies\n",
    "\n",
    "In the next cell, we'll:\n",
    "1. Import required PyTorch and HuggingFace libraries\n",
    "2. Download and load a [next token prediction dataset](https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset) from HuggingFace\n",
    "3. Create PyTorch DataLoaders for training\n",
    "4. Calculate vocabulary size for the model\n",
    "5. Define a multilayer model for training\n",
    "\n",
    "_You can modify this setup to use your own data and model architecture._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created: SimpleMNISTModel\n",
      "Optimizer: Adam\n",
      "Criterion: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "## Before you start\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def setup_dataset(params: dict, data_files):\n",
    "    # Download dataset\n",
    "    base_url = \"https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset/resolve/main/data/\"\n",
    "\n",
    "    if isinstance(data_files, str):\n",
    "        data_files = [data_files]\n",
    "\n",
    "    data_files = {\n",
    "        \"train\": [base_url + file for file in data_files]\n",
    "    }\n",
    "\n",
    "    # Load dataset\n",
    "    data_subset = load_dataset(\"parquet\", data_files=data_files, num_proc=4)\n",
    "    # validation_subset = data_subset.get(\"validation\").train_test_split(test_size=0.1)\n",
    "\n",
    "    print(f\"Training samples: {data_subset['train'].num_rows}\")\n",
    "    # print(f\"Validation samples: {validation_subset['test'].num_rows}\")\n",
    "\n",
    "    # Convert to PyTorch format\n",
    "    train_subset = data_subset[\"train\"].with_format(\n",
    "        type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    "    )\n",
    "    \"\"\"validation_subset = validation_subset[\"test\"].with_format(\n",
    "        type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    "    )\"\"\"\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    # val_dataloader = DataLoader(validation_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # Calculate vocabulary size\n",
    "    params[\"vocab_size\"] = (\n",
    "        max([token for sentence in data_subset[\"train\"][\"input_ids\"] for token in sentence]) + 1\n",
    "    )\n",
    "    print(f\"Vocabulary size: {params['vocab_size']}\")\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "def create_mnist_dataloader(params):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the MNIST dataset.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): Number of samples per batch\n",
    "        num_workers (int): Number of subprocesses to use for data loading\n",
    "        shuffle (bool): Whether to shuffle the data\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader for MNIST training data\n",
    "    \"\"\"\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "    ])\n",
    "    \n",
    "    # Download and load training data\n",
    "    train_dataset = datasets.MNIST(\n",
    "        root='./data',  # Directory to save the data\n",
    "        train=True,     # Use training set\n",
    "        download=True,  # Download if not already present\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    return train_loader\n",
    "\n",
    "# Working with PyTorch checkpoints\n",
    "def save_model_state(model, optimizer, step_counter, epoch):\n",
    "    \n",
    "    save_path=f\"model_state_step_{step_counter}_epoch_{epoch}.pt\"\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'step': step_counter\n",
    "    }, save_path)\n",
    "\n",
    "def load_model_state(model, optimizer, step_counter, epoch):\n",
    "\n",
    "    load_path=f\"model_state_step_{step_counter}_epoch_{epoch}.pt\"\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint\n",
    "\n",
    "class SimpleMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMNISTModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Max pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 7*7 comes from image size after pooling\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 classes for MNIST\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "class MultilayerModel(nn.Module):\n",
    "    \"\"\"A larger language model with multiple LSTM and fully connected layers.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(MultilayerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Create multiple LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.LSTM(\n",
    "                    hidden_size if i > 0 else embed_size,\n",
    "                    hidden_size,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "                for i in range(10)  # 10 LSTM layers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create multiple fully connected layers\n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, hidden_size) for _ in range(9)]  # 9 FC layers\n",
    "        )\n",
    "\n",
    "        # Final layer to project back to vocab size\n",
    "        self.final_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Process through LSTM layers\n",
    "        for lstm in self.lstm_layers:\n",
    "            x, _ = lstm(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Process through FC layers\n",
    "        for fc in self.fc_layers:\n",
    "            x = fc(x)\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # Final projection\n",
    "        out = self.final_layer(x)\n",
    "        return out\n",
    "\n",
    "dataset_file = \"train-00001-of-00067.parquet\"\n",
    "train_dataloader = create_mnist_dataloader(params)\n",
    "'''model = MultilayerModel(\n",
    "    params[\"vocab_size\"],\n",
    "    params[\"embed_size\"],\n",
    "    params[\"hidden_size\"],\n",
    ")'''\n",
    "model = SimpleMNISTModel()\n",
    "model.to(params[\"device\"])\n",
    "print(f\"Model created: {model.__class__.__name__}\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=-100\n",
    ")  # Ignore the buffering index of -100 in the dataset\n",
    "print(f\"Criterion: {criterion.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug model training run with Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: _Initialize Neptune Run object_\n",
    "\n",
    "The `Run` object is used to log configuration parameters and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=forking-text&type=experiment\n"
     ]
    }
   ],
   "source": [
    "from neptune_scale import Run\n",
    "\n",
    "run = Run(\n",
    "    # run_id = \"base-run\", # Example of a custom run_id and illustration of this example\n",
    "    experiment_name=\"forking-text\",  # Create a run that is the head of an experiment.\n",
    ")\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: _Log configuration parameters and tags_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See configuration parameters:\n",
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=forking-text&type=experiment&detailsTab=metadata\n"
     ]
    }
   ],
   "source": [
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/num_lstm_layers\": params[\"num_lstm_layers\"],\n",
    "        \"data/embed_size\": params[\"embed_size\"],\n",
    "        \"sys/description\": \"Base experiment used to get an inital version of the model\"\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[\"text\", \"forking\", params[\"optimizer\"]])\n",
    "run.add_tags([dataset_file], group_tags=True)\n",
    "\n",
    "print(f\"See configuration parameters:\\n{run.get_experiment_url() + '&detailsTab=metadata'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: _Execute model training loop_\n",
    "\n",
    "In this training loop, we:\n",
    "1. Register backward hooks to capture gradient norms from all model layers with `register_full_backward_hook()`\n",
    "2. Track these norms during training to identify potential issues like vanishing/exploding gradients in a dictionary called `debugging_gradient_norms`\n",
    "3. Log the gradient norms to Neptune for visualization and analysis using the `log_metrics` method\n",
    "\n",
    "This approach allows you to monitor the learning dynamics across your entire model architecture in near real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View charts in near real-time:\n",
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=forking-text&type=experiment&detailsTab=charts\n",
      "Saving model checkpoint epoch: 0\n",
      "Saving model checkpoint epoch: 1\n",
      "Saving model checkpoint epoch: 2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Register hooks to track gradients for each layer\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/num_lstm_layers\": params[\"num_lstm_layers\"],\n",
    "        \"data/embed_size\": params[\"embed_size\"],\n",
    "\n",
    "# Define dictionary of metrics to log to Neptune\n",
    "debugging_gradient_norms = {}\n",
    "# Register hooks once before training\n",
    "for name, module in model.named_modules():\n",
    "    module.register_full_backward_hook(hook_fn)\n",
    "    '''\n",
    "\n",
    "# Create custom Neptune URLS for tutorial steps\n",
    "print(f\"View charts in near real-time:\\n{run.get_experiment_url() + '&detailsTab=charts'}\")\n",
    "\n",
    "step_counter = 0\n",
    "# Training loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        step_counter += 1\n",
    "        # input_ids = batch[0].to(params[\"device\"])\n",
    "        images = batch[0].to(params[\"device\"])\n",
    "        labels = batch[1].to(params[\"device\"])\n",
    "        optimizer.zero_grad()\n",
    "        # logits = model(input_ids)\n",
    "        # loss = criterion(logits.view(-1, params[\"vocab_size\"]), labels.view(-1))\n",
    "        outputs = model(images)  # No need for .view() with MNIST\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log global training loss and layer-wise gradient norms\n",
    "        run.log_metrics(\n",
    "            data={\"metrics/train/loss\": loss.item()},\n",
    "            step=step_counter,\n",
    "        )\n",
    "\n",
    "    print(f\"Saving model checkpoint epoch: {epoch}\")\n",
    "    save_model_state(model, optimizer, step_counter, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:57:02,317 neptune:INFO: Waiting for all operations to be processed\n",
      "2025-04-14 17:57:02,320 neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "2025-04-14 17:57:02,321 neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Close run to ensure all operations are processed\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 163852\n",
      "Vocabulary size: 128257\n",
      "Model created: MultilayerModel\n",
      "Optimizer: Adam\n",
      "Criterion: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "# Need to update parameters, model architecture or dataset\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"input_features\": 256,\n",
    "    \"dropout_prob\": 0.3,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}\n",
    "\n",
    "# Add additional dataset\n",
    "dataset_file = [\"train-00001-of-00067.parquet\", \"train-00002-of-00067.parquet\"]\n",
    "train_dataloader = setup_dataset(params, dataset_file)\n",
    "\n",
    "'''\n",
    "model = MultilayerModel(\n",
    "    params[\"vocab_size\"],\n",
    "    params[\"embed_size\"],\n",
    "    params[\"hidden_size\"],\n",
    ")'''\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "# Load model state from saved checkpoint\n",
    "load_model_state(model, optimizer, step_counter=100, epoch=0)\n",
    "\n",
    "# Use a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.1,\n",
    "    patience=3.5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.to(params[\"device\"])\n",
    "print(f\"Model created: {model.__class__.__name__}\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=-100\n",
    ")  # Ignore the buffering index of -100 in the dataset\n",
    "print(f\"Criterion: {criterion.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fortified-frequency-20250414160234875-75smj\n",
      "https://scale.neptune.ai/leo/pytorch-tutorial/-/run/?customId=industrial-hypothesis-20250414160311411-wv20s\n"
     ]
    }
   ],
   "source": [
    "# Create a forked run\n",
    "print(run._run_id)\n",
    "run = Run(\n",
    "    #experiment_name=\"forking-text\",\n",
    "    # run_id=\"forked-run\",\n",
    "    fork_run_id = \"creative-batch-20250414154819921-69jt8\",\n",
    "    fork_step = 100\n",
    ")\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# run.add_tags(tags=[\"text\", \"forking\", params[\"optimizer\"]])\n",
    "run.add_tags(dataset_file, group_tags=True)\n",
    "\n",
    "print(run.get_run_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Log global training loss and layer-wise gradient norms\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step_counter = 101\n",
    "# Training loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        step_counter += 1\n",
    "        input_ids = batch[\"input_ids\"].to(params[\"device\"])\n",
    "        labels = batch[\"labels\"].to(params[\"device\"])\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits.view(-1, params[\"vocab_size\"]), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate based on step\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Log global training loss and layer-wise gradient norms\n",
    "        run.log_metrics(\n",
    "            data={\"metrics/train/loss\": loss.item()},\n",
    "            step=step_counter,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 18:08:09,170 neptune:INFO: Waiting for all operations to be processed\n",
      "2025-04-14 18:08:09,172 neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "2025-04-14 18:08:09,173 neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: _Analyze and debug training_\n",
    "While the model is training, you can start using the Neptune web app to browse your metrics and create custom analyses and visualizations:\n",
    "1. To visualize the large number of metrics being logged in near real time, navigate to the **Charts** tab of the active run (_or select link above_).\n",
    "2. Filter the metrics using the [advanced regex searching capabilities](https://docs-beta.neptune.ai/charts#filtering-charts). For example, enter `gradient & fc & layers.[0-5] & norm` in the search bar. This query filters the metrics for the first 6 layers of the gradients norms of the fully connected layers. You can specify down to exactly the metrics name you want.\n",
    "\n",
    "![Alt text](tutorial-images/debugging_regex_search.png)\n",
    "\n",
    "3. Export the filter to a [dashboard](https://docs-beta.neptune.ai/custom_dashboard). The saved dashboard will now only display these metrics during training. This is useful if you know that a certain set of layers can be troublesome during training. \n",
    "4. Alternatively, use the [dynamic metric selection](https://docs-beta.neptune.ai/chart_widget#dynamic-metric-selection) and create a new chart widget to display all LSTM layers gradient norms in one chart. Again, use the `(.*gradient)(.*lstm)(.*norm)` query. This makes it easy to have an automatically updating chart that allows you to view all layers on a single chart for rapid debugging in case vanishing or exploding gradients appear. \n",
    "\n",
    "![Alt text](tutorial-images/debugging_dashboard.png)\n",
    "\n",
    "5. To document this behavior, create a [custom report](https://docs-beta.neptune.ai/reports) to outline the model training, global metrics, debugging metrics for the model you're training. This allows you to keep track of any anomalies but also to see what worked or did not work during training.\n",
    "\n",
    "![Alt text](tutorial-images/debugging_report.png)\n",
    "\n",
    "See the pre-configured [example of the training report](https://scale.neptune.ai/leo/pytorch-tutorial/reports/9e79d952-272a-4a38-83e5-27df4dd225ec).\n",
    "\n",
    "See also: PyTorch layer-wise tracking package [here](TODO:Link to integration for tracking layer-wise metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
