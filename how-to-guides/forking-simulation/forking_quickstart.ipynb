{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune Advanced Quickstart (with forking)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lb/advanced_quickstart/how-to-guides/advanced_quickstart/neptune_scale_quickstart.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide shows how to:\n",
    "- **Create a Neptune run**\n",
    "- **Log training metrics to the run**\n",
    "\n",
    "We will also demonstrate how Neptune can be used for large scale training, by logging 66 unique metrics for 20000 steps each. This demonstrates a scale of logging ~1.3M datapoints. These values can be adjusted to see the ingestion and UI responsiveness of Neptune. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Neptune and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U neptune_scale numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from neptune_scale import Run\n",
    "import time\n",
    "import os\n",
    "from tqdm import trange\n",
    "\n",
    "from training_simulation import *  # code that simulates neural network training metrics without actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and set your API token\n",
    "\n",
    "If you haven't already, [create a project](https://docs-beta.neptune.ai/setup#1-create-a-project).\n",
    "\n",
    "To find your API token and full project name:\n",
    "1. Log into Neptune Scale.\n",
    "2. In the bottom-left corner, expand your user menu and select **Get your API token**.\n",
    "3. Using your token, set the `NEPTUNE_API_TOKEN` environment variable before running this notebook.\n",
    "4. To find the full project name, open the project settings. Copy and paste the project path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEPTUNE_PROJECT\"] = \"copy-paste-your-project-id-from-neptune-ui-here\"\n",
    "print(os.environ[\"NEPTUNE_PROJECT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Neptune run\n",
    "\n",
    "To initialize the `Run` object, you must provide a unique `run_id` to identify your experiment. You can also pass your API token and project name as arguments to the `Run` constructor (as seen in the pseudo code below), but these have already been set as environment variables.\n",
    "\n",
    "```python\n",
    "run = Run(\n",
    "    api_token = \"YOUR_API_TOKEN\",\n",
    "    project = \"YOUR_PROJECT_NAME/YOUR_WORKSPACE_NAME\",\n",
    "    run_id = \"UNIQUE_RUN_IDENTIFIER\"\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "To log any configuration parameters or single values like `learning rate`, `batch size` or `optimizer`, use the `run.log_configs()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_index = random.randint(0, 10_000)\n",
    "custom_id = f\"quickstart-{run_index}\"  # Sets a random value for the custom run_id\n",
    "\n",
    "run = Run(\n",
    "    experiment_name=custom_id,  # This run becomes the head of an experiment\n",
    "    run_id=custom_id,  # You can customize your run_id, but if not specified, will be generated automatically\n",
    ")\n",
    "\n",
    "# Add any tags to identify your runs\n",
    "run.add_tags([\"Quickstart\", \"Long\"])\n",
    "run.add_tags([\"Notebook\"], group_tags=True)\n",
    "\n",
    "parameters = get_parameters(run_index=run_index, n_steps=100_000)\n",
    "allowed_datatypes = [int, float, str, datetime, bool]\n",
    "run.log_configs(\n",
    "    {\n",
    "        **{f\"parameters/model/{k}\": v if v in allowed_datatypes else str(v) for k, v in parameters[\"model\"].items()},\n",
    "        **{f\"parameters/optimizer/{k}\": v if v in allowed_datatypes else str(v) for k, v in parameters[\"optimizer\"].items()},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training loop that logs to Neptune\n",
    "\n",
    "This training loop tracks 66 different training metrics each for 20 000 steps. You can increase the number of metrics in the logging dictionary as well as the number of steps.\n",
    "\n",
    "```python\n",
    "metrics_to_log = {\n",
    "    \"metric_1\": metric_1,\n",
    "    ... ,\n",
    "    \"metric_x\": metric_x\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "To find the run in the Neptune web app, navigate to the [**All runs**](https://scale.neptune.ai/) tab. Next to the search bar, enable the **Show all runs** toggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Time length of execution\n",
    "logging_time = 0\n",
    "\n",
    "# Simulate the training loop\n",
    "model_state = simulate_init_new_model(parameters)\n",
    "optimizer_state = simulate_init_optimizer()\n",
    "for step in trange(1, parameters[\"training\"][\"steps\"] + 1):\n",
    "    # simulate training step\n",
    "    model_state, optimizer_state, loss, accuracy = simulate_training_step(model_state, optimizer_state, step, parameters)\n",
    "    metrics = {\n",
    "        \"train/metrics/loss\": loss,\n",
    "        \"train/metrics/accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "    # capture debugging metrics for each layer of the model\n",
    "    for layer_idx, layer_state in enumerate(model_state[\"layers\"]):\n",
    "        metrics[f\"metrics/layer/{layer_idx:02d}/activation_mean\"] = layer_state[\"activation_mean\"]\n",
    "        metrics[f\"metrics/layer/{layer_idx:02d}/gradient_norm\"] = layer_state[\"gradient_norm\"]\n",
    "        \n",
    "    # simulate evaluation step\n",
    "    eval_loss, eval_accuracy, eval_bleu, eval_wer = simulate_eval_step(model_state, step, parameters)\n",
    "    metrics[\"test/metrics/loss\"] = eval_loss\n",
    "    metrics[\"test/metrics/accuracy\"] = eval_accuracy\n",
    "    metrics[\"test/metrics/bleu\"] = eval_bleu\n",
    "    metrics[\"test/metrics/wer\"] = eval_wer\n",
    "\n",
    "    # Simulate hardware metrics\n",
    "    for i in range(10):\n",
    "        metrics[f\"hardware/gpu_{i}\"] = random.random()\n",
    "    \n",
    "    # Log metrics usig the run.log_metrics() method\n",
    "    logging_time_start = time.time()\n",
    "    run.log_metrics(data=metrics, step=step)\n",
    "    logging_time_end = time.time()\n",
    "    logging_time += logging_time_end - logging_time_start\n",
    "\n",
    "    # save checkpoint every 100 steps\n",
    "    save_checkpoint(custom_id, step, model_state, optimizer_state, parameters)\n",
    "\n",
    "# Close run and ensure all operations are processed\n",
    "run.close()\n",
    "\n",
    "# Calculate some post run metrics for review\n",
    "num_ops = parameters[\"training\"][\"steps\"] * len(metrics)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Unique metrics per run: {len(metrics)}\")\n",
    "print(f'Number of steps per run: {parameters[\"training\"][\"steps\"]}')\n",
    "print(f\"Total data points logged per run: {num_ops}\")\n",
    "print(\n",
    "    f\"Total execution time: {execution_time:.2f} seconds to process {num_ops} operations ({num_ops/execution_time:.0f} datapoints/second).\"\n",
    ")\n",
    "print(f\"Logging time {logging_time}, ({num_ops/logging_time:.0f} datapoints/second).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forking\n",
    "\n",
    "To fork the run, all you need is the parent run id & fork step at which the new run will be created.\n",
    "\n",
    "In most scenarios, you'll want these to correspond to the checkpoint from specific step in one of the previous runs.\n",
    "\n",
    "Forking is useful especially if you want to keep the original experiment running in parallel to the new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fork_at_step = 70_000\n",
    "forked_run = Run(\n",
    "    experiment_name=f\"{custom_id}-forked\",\n",
    "    run_id=f\"{custom_id}-forked\",\n",
    "    fork_run_id=custom_id, # id of the parent run\n",
    "    fork_step=70_000, # checkpoint step from which we're going to start the forked run\n",
    ")\n",
    "\n",
    "# change tags & configs if you want to\n",
    "forked_run.add_tags([\"Forked\"])\n",
    "forked_run.log_configs(\n",
    "    {\n",
    "        \"parameters/optimizer/lr\": 0.001,\n",
    "        \"parameters/model/batch_size\": 32,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(forked_run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Time length of execution\n",
    "logging_time = 0\n",
    "\n",
    "# Simulate the training loop\n",
    "checkpoint = load_checkpoint(\n",
    "    checkpoint_path=f\"./{checkpoint_path(custom_id, fork_at_step)}\",\n",
    "    parameters=parameters\n",
    ")\n",
    "model_state = checkpoint[\"model_state\"]\n",
    "model_state[\"simulation_behavior\"] = \"fast_convergence\"\n",
    "model_state[\"simulation_behavior_start_step\"] = model_state[\"simulation_behavior_start_step\"] * 2\n",
    "optimizer_state = checkpoint[\"optimizer_state\"]\n",
    "for step in trange(fork_at_step, parameters[\"training\"][\"steps\"] + fork_at_step + 1):\n",
    "    # simulate training step\n",
    "    model_state, optimizer_state, loss, accuracy = simulate_training_step(model_state, optimizer_state, step, parameters)\n",
    "    metrics = {\n",
    "        \"train/metrics/loss\": loss,\n",
    "        \"train/metrics/accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "    # capture debugging metrics for each layer of the model\n",
    "    for layer_idx, layer_state in enumerate(model_state[\"layers\"]):\n",
    "        metrics[f\"metrics/layer/{layer_idx:02d}/activation_mean\"] = layer_state[\"activation_mean\"]\n",
    "        metrics[f\"metrics/layer/{layer_idx:02d}/gradient_norm\"] = layer_state[\"gradient_norm\"]\n",
    "        \n",
    "    # simulate evaluation step\n",
    "    eval_loss, eval_accuracy, eval_bleu, eval_wer = simulate_eval_step(model_state, step, parameters)\n",
    "    metrics[\"test/metrics/loss\"] = eval_loss\n",
    "    metrics[\"test/metrics/accuracy\"] = eval_accuracy\n",
    "    metrics[\"test/metrics/bleu\"] = eval_bleu\n",
    "    metrics[\"test/metrics/wer\"] = eval_wer\n",
    "\n",
    "    # Simulate hardware metrics\n",
    "    for i in range(10):\n",
    "        metrics[f\"hardware/gpu_{i}\"] = random.random()\n",
    "    \n",
    "    # Log metrics usig the run.log_metrics() method\n",
    "    logging_time_start = time.time()\n",
    "    forked_run.log_metrics(data=metrics, step=step)\n",
    "    logging_time_end = time.time()\n",
    "    logging_time += logging_time_end - logging_time_start\n",
    "\n",
    "    # print(f\"progress={100 * step/steps:.1f}%\")\n",
    "\n",
    "# Close run and ensure all operations are processed\n",
    "forked_run.close()\n",
    "\n",
    "# Calculate some post run metrics for review\n",
    "num_ops = parameters[\"training\"][\"steps\"] * len(metrics)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Unique metrics per run: {len(metrics)}\")\n",
    "print(f'Number of steps per run: {parameters[\"training\"][\"steps\"]}')\n",
    "print(f\"Total data points logged per run: {num_ops}\")\n",
    "print(\n",
    "    f\"Total execution time: {execution_time:.2f} seconds to process {num_ops} operations ({num_ops/execution_time:.0f} datapoints/second).\"\n",
    ")\n",
    "print(f\"Logging time {logging_time}, ({num_ops/logging_time:.0f} datapoints/second).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
