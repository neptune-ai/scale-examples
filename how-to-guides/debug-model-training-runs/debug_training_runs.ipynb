{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Model Training with Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lb/debugging_model_training/how-to-guides/debug-model-training-runs/debug_training_runs.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Training large models requires careful monitoring of layer-wise metrics to catch issues early. \n",
    "\n",
    "Neptune makes it easy to track and visualize metrics like gradient norms across all layers of your model - helping you identify problems like vanishing/exploding gradients quickly.\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "1. **Initialize Neptune** and **log configuration parameters**\n",
    "2. Track **layer-wise gradient norms** during training \n",
    "3. Analyze the metrics in Neptune's UI to **debug training issues**\n",
    "\n",
    "Step through a pre-configured report [here](https://scale.neptune.ai/leo/pytorch-tutorial/reports/9e79d952-272a-4a38-83e5-27df4dd225ec) to see a finalized version.\n",
    "\n",
    "_Note: This is a code recipe that you can adapt for your own model training needs._\n",
    "\n",
    " ![Layer-wise gradient norms visualization in Neptune](tutorial-images/debugging_report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "Set your project name and API token as environment variables to log to your Neptune Scale project.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = YOUR_API_TOKEN\n",
    "# %env NEPTUNE_PROJECT = WORKSPACE_NAME/PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale torch datasets\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"embed_size\": 1000,\n",
    "    \"hidden_size\": 256,  # hidden size for the LSTM\n",
    "    \"dropout_prob\": 0.3,\n",
    "    \"num_lstm_layers\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data, model and other dependencies\n",
    "\n",
    "In the next cell, we'll:\n",
    "1. Import required PyTorch and HuggingFace libraries\n",
    "2. Download and load a [next token prediction dataset](https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset) from HuggingFace\n",
    "3. Create PyTorch DataLoaders for training\n",
    "4. Calculate vocabulary size for the model\n",
    "5. Define a multilayer model for training\n",
    "\n",
    "You can modify this setup to use your own data and model architecture if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 81926\n",
      "Vocabulary size: 128257\n",
      "Model created: MultilayerModel\n",
      "Optimizer: Adam\n",
      "Criterion: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Download dataset\n",
    "base_url = \"https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset/resolve/main/data/\"\n",
    "data_files = {\n",
    "    \"train\": base_url + \"train-00001-of-00067.parquet\",\n",
    "    \"validation\": base_url + \"validation-00000-of-00001.parquet\",\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "data_subset = load_dataset(\"parquet\", data_files=data_files, num_proc=4)\n",
    "# validation_subset = data_subset.get(\"validation\").train_test_split(test_size=0.1)\n",
    "\n",
    "print(f\"Training samples: {data_subset['train'].num_rows}\")\n",
    "# print(f\"Validation samples: {validation_subset['test'].num_rows}\")\n",
    "\n",
    "# Convert to PyTorch format\n",
    "train_subset = data_subset[\"train\"].with_format(\n",
    "    type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    ")\n",
    "\"\"\"validation_subset = validation_subset[\"test\"].with_format(\n",
    "    type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    ")\"\"\"\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "# val_dataloader = DataLoader(validation_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Calculate vocabulary size\n",
    "params[\"vocab_size\"] = (\n",
    "    max([token for sentence in data_subset[\"train\"][\"input_ids\"] for token in sentence]) + 1\n",
    ")\n",
    "print(f\"Vocabulary size: {params['vocab_size']}\")\n",
    "\n",
    "\n",
    "# Create model\n",
    "class MultilayerModel(nn.Module):\n",
    "    \"\"\"A larger language model with multiple LSTM and fully connected layers.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(MultilayerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Create multiple LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.LSTM(\n",
    "                    hidden_size if i > 0 else embed_size,\n",
    "                    hidden_size,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "                for i in range(10)  # 10 LSTM layers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Create multiple fully connected layers\n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, hidden_size) for _ in range(9)]  # 9 FC layers\n",
    "        )\n",
    "\n",
    "        # Final layer to project back to vocab size\n",
    "        self.final_layer = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Process through LSTM layers\n",
    "        for lstm in self.lstm_layers:\n",
    "            x, _ = lstm(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Process through FC layers\n",
    "        for fc in self.fc_layers:\n",
    "            x = fc(x)\n",
    "            x = self.dropout(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # Final projection\n",
    "        out = self.final_layer(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = MultilayerModel(\n",
    "    params[\"vocab_size\"],\n",
    "    params[\"embed_size\"],\n",
    "    params[\"hidden_size\"],\n",
    ")\n",
    "print(f\"Model created: {model.__class__.__name__}\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=-100\n",
    ")  # Ignore the buffering index of -100 in the dataset\n",
    "print(f\"Criterion: {criterion.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug model training run with Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: _Initialize Neptune Run object_\n",
    "\n",
    "The `Run` object is used to log configuration parameters and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune_scale import Run\n",
    "\n",
    "run = Run(\n",
    "    experiment_name=\"pytorch-text\",  # Create a run that is the head of an experiment. This is also used for forking.\n",
    ")\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: _Log configuration parameters and tags_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/num_lstm_layers\": params[\"num_lstm_layers\"],\n",
    "        \"data/embed_size\": params[\"embed_size\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[\"text\", \"LLM\", \"Simple\", params[\"optimizer\"]])\n",
    "\n",
    "print(f\"See configuration parameters:\\n{run.get_experiment_url() + '&detailsTab=metadata'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: _Execute model training loop_\n",
    "\n",
    "In this training loop, we:\n",
    "1. Register backward hooks to capture gradient norms from all model layers with `register_full_backward_hook()`\n",
    "2. Track these norms during training to identify potential issues like vanishing/exploding gradients in a dictionary called `debugging_gradient_norms`\n",
    "3. Log the gradient norms to Neptune for visualization and analysis using the `log_metrics` method\n",
    "\n",
    "This approach allows you to monitor the learning dynamics across your entire model architecture in near real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks to track gradients for each layer\n",
    "def hook_fn(module, grad_input, grad_output):\n",
    "    layer_name = next(name for name, mod in model.named_modules() if mod is module)\n",
    "    if grad_input[0] is not None:  # Check if gradients exist\n",
    "        grad_norm = grad_input[0].norm().item()\n",
    "        debugging_gradient_norms[f\"debug/gradient/{layer_name}/norm\"] = grad_norm\n",
    "\n",
    "\n",
    "# Define dictionary of metrics to log to Neptune\n",
    "debugging_gradient_norms = {}\n",
    "# Register hooks once before training\n",
    "for name, module in model.named_modules():\n",
    "    module.register_full_backward_hook(hook_fn)\n",
    "\n",
    "# Create custom Neptune URLS for tutorial steps\n",
    "print(f\"View charts in near real-time:\\n{run.get_experiment_url() + '&detailsTab=charts'}\")\n",
    "\n",
    "step_counter = 0\n",
    "# Training loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        step_counter += 1\n",
    "        input_ids = batch[\"input_ids\"].to(params[\"device\"])\n",
    "        labels = batch[\"labels\"].to(params[\"device\"])\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits.view(-1, params[\"vocab_size\"]), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log global training loss and layer-wise gradient norms\n",
    "        run.log_metrics(\n",
    "            data={\"metrics/train/loss\": loss.item(), **debugging_gradient_norms},\n",
    "            step=step_counter,\n",
    "        )\n",
    "\n",
    "# Close run to ensure all operations are processed\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: _Analyze and debug training_\n",
    "While the model is training, you can start using the Neptune web app to browse your metrics and create custom analyses and visualizations:\n",
    "1. To visualize the large number of metrics being logged in near real time, navigate to the **Charts** tab of the active run (_or select link above_).\n",
    "2. Filter the metrics using the [advanced regex searching capabilities](https://docs-beta.neptune.ai/charts#filtering-charts). For example, enter `gradient & fc & layers.[0-5] & norm` in the search bar. This query filters the metrics for the first 6 layers of the gradients norms of the fully connected layers. You can specify down to exactly the metrics name you want.\n",
    "\n",
    "![Alt text](tutorial-images/debugging_regex_search.png)\n",
    "\n",
    "3. Export the filter to a [dashboard](https://docs-beta.neptune.ai/custom_dashboard). The saved dashboard will now only display these metrics during training. This is useful if you know that a certain set of layers can be troublesome during training. \n",
    "4. Alternatively, use the [dynamic metric selection](https://docs-beta.neptune.ai/chart_widget#dynamic-metric-selection) and create a new chart widget to display all LSTM layers gradient norms in one chart. Again, use the `(.*gradient)(.*lstm)(.*norm)` query. This makes it easy to have an automatically updating chart that allows you to view all layers on a single chart for rapid debugging in case vanishing or exploding gradients appear. \n",
    "\n",
    "![Alt text](tutorial-images/debugging_dashboard.png)\n",
    "\n",
    "5. To document this behavior, create a [custom report](https://docs-beta.neptune.ai/reports) to outline the model training, global metrics, debugging metrics for the model you're training. This allows you to keep track of any anomalies but also to see what worked or did not work during training.\n",
    "\n",
    "![Alt text](tutorial-images/debugging_report.png)\n",
    "\n",
    "See the pre-configured [example of the training report](https://scale.neptune.ai/leo/pytorch-tutorial/reports/9e79d952-272a-4a38-83e5-27df4dd225ec).\n",
    "\n",
    "See also: PyTorch layer-wise tracking package [here](TODO:Link to integration for tracking layer-wise metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
