{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Model Training Runs with Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lb/debugging_model_training/how-to-guides/debug-model-training-runs/debug_training_runs.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/neptune-ai/scale-examples/blob/lb/debugging_model_training/how-to-guides/debug-model-training-runs/debug_training_runs.ipynb\">\n",
    "  <img alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open_in_GitHub-blue?logo=github&labelColor=black\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://docs-beta.neptune.ai/tutorials/\">\n",
    "  <img alt=\"View tutorial in docs\" src=\"https://neptune.ai/wp-content/uploads/2024/01/docs-badge-2.svg\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Training large models requires careful monitoring of layer-wise metrics to catch issues early. \n",
    "\n",
    "Neptune makes it easy to track and visualize metrics like gradient norms across all layers of your model - helping you identify problems like vanishing/exploding gradients quickly.\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "1. **Initialize Neptune** and **log configuration parameters**\n",
    "2. Track **layer-wise gradient norms** during training \n",
    "3. Analyze the metrics in Neptune's UI to **debug training issues**\n",
    "\n",
    "Step through a pre-configured report:\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/leo/pytorch-tutorial/reports/9ed051fc-9b16-4c31-a2da-925230c9c360\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "\n",
    "_Note: This is a code recipe that you can adapt for your own model training needs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "Set your project name and API token as environment variables to log to your Neptune Scale project.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = YOUR_API_TOKEN\n",
    "# %env NEPTUNE_PROJECT = WORKSPACE_NAME/PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries\n",
    "\n",
    "This tutorial uses a datset from Hugging Face which is loaded using the `datasets` package from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training simulation\n",
    "\n",
    "This tutorial uses a simulated training scenario to demonstrate Neptune's debugging capabilities. The simulation generates synthetic training metrics that mimic real model training, including:\n",
    "- Gradually decreasing loss\n",
    "- Increasing accuracy\n",
    "- Random noise and occasional spikes to mimic gradient norm explosions\n",
    "- Per-layer gradient norms that fluctuate naturally and spike during loss anomalies\n",
    "\n",
    "In the next cell, we'll:\n",
    "1. Import required libraries (numpy, json, etc.) \n",
    "2. Create a `TrainingMetrics` class that simulates training progression\n",
    "3. Track gradient norms for 20 layers to help identify optimization issues\n",
    "\n",
    "_You can replace this simulation with your actual model training code._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "import math\n",
    "\n",
    "\n",
    "class TrainingMetrics:\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_loss: float = 1.0,\n",
    "        initial_accuracy: float = 0.0,\n",
    "        noise_scale: float = 0.1,\n",
    "        loss_trend: float = -0.1,\n",
    "        accuracy_trend: float = 0.1,\n",
    "    ):\n",
    "        self.previous_loss = initial_loss\n",
    "        self.previous_accuracy = initial_accuracy\n",
    "        self.noise_scale = noise_scale\n",
    "        self.loss_trend = loss_trend\n",
    "        self.accuracy_trend = accuracy_trend\n",
    "        self.step_count = 1\n",
    "\n",
    "        # Random convergence points\n",
    "        self.target_loss = np.random.uniform(0.0, 1.0)\n",
    "        self.target_accuracy = np.random.uniform(0.0, 1.0)\n",
    "\n",
    "    def update_metrics(self, spikes=True) -> tuple[float, float, dict]:\n",
    "        \"\"\"Update loss and accuracy using a random walk process with logarithmic trends\"\"\"\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Base loss update with normal progression\n",
    "        decay_factor = math.log(1 + abs(self.previous_loss - self.target_loss))\n",
    "        loss_step = self.noise_scale * np.random.randn() + self.loss_trend * decay_factor\n",
    "        loss_step *= 1 + 0.1 * math.log(self.step_count)\n",
    "\n",
    "        # Initialize gradient norms for 20 layers with small random values\n",
    "        gradient_norms = {\n",
    "            f\"debug/grad_norm/layer_{i}\": np.random.uniform(0.01, 0.1) for i in range(20)\n",
    "        }\n",
    "\n",
    "        # Check for spike/anomaly (0.01% chance) after step 10k\n",
    "        if spikes and np.random.random() < 0.0001:  # and self.step_count > 10000:\n",
    "            # Generate a sudden spike, independent of current loss\n",
    "            spike_magnitude = np.random.uniform(1, 10)  # Random spike between 1x and 5x\n",
    "            current_loss = spike_magnitude\n",
    "\n",
    "            # When there's a loss spike, create a corresponding spike in a random layer\n",
    "            spike_layer = np.random.randint(0, 20)\n",
    "            gradient_norms[f\"debug/grad_norm/layer_{spike_layer}\"] = (\n",
    "                spike_magnitude * np.random.uniform(0.8, 1.2)\n",
    "            )\n",
    "        else:\n",
    "            # Normal progression\n",
    "            current_loss = max(0.0, self.previous_loss + loss_step)\n",
    "            self.previous_loss = current_loss\n",
    "\n",
    "        current_accuracy = 1 - current_loss\n",
    "\n",
    "        return current_loss, current_accuracy, gradient_norms\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"noise_scale\": np.random.uniform(0.0005, 0.002),\n",
    "    \"loss_trend\": -np.random.uniform(0, 0.0002),\n",
    "    \"accuracy_trend\": np.random.uniform(0, 0.0002),\n",
    "}\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = TrainingMetrics(\n",
    "    initial_loss=np.random.uniform(3, 7),\n",
    "    initial_accuracy=0,\n",
    "    noise_scale=params[\"noise_scale\"],\n",
    "    loss_trend=params[\"loss_trend\"],\n",
    "    accuracy_trend=params[\"accuracy_trend\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug model training run with Neptune\n",
    "\n",
    "In this section, we'll walk through 4 key steps to effectively debug your training runs:\n",
    "\n",
    "1. **Initialize Neptune** - set up the Neptune environment to track your training metrics\n",
    "2. **Log configuration parameters** - record your model's hyperparameters and setup\n",
    "3. **Track layer-wise metrics** - monitor gradient norms across all model layers\n",
    "4. **Analyze training behavior** - Use Neptune's visualization tools to identify and diagnose training issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: _Initialize Neptune Run object_\n",
    "\n",
    "The `Run` object is used to log configuration parameters and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune_scale import Run\n",
    "\n",
    "run = Run(\n",
    "    experiment_name=\"debugging-gradient-norms\",  # Create a run that is the head of an experiment.\n",
    ")\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: _Log configuration parameters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_configs(\n",
    "    {\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[\"debug\", \"gradient-norm\"])\n",
    "\n",
    "print(f\"See configuration parameters:\\n{run.get_experiment_url() + '&detailsTab=metadata'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: _Track gradient norms during training_\n",
    "\n",
    "In this training loop, we:\n",
    "1. Simulate the calculation of `loss`, `accuracy` and `gradient norms`\n",
    "2. Track gradient norms during training to identify potential issues like vanishing/exploding gradients in a dictionary called `gradient_norms`\n",
    "3. Log the gradient norms to Neptune for visualization and analysis using the `log_metrics` method\n",
    "\n",
    "This approach allows you to monitor the learning dynamics across your entire model architecture in near real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counter = 0\n",
    "# Training loop\n",
    "for epoch in range(1, params[\"epochs\"]):\n",
    "    # Process batches\n",
    "    for batch in range(0, 10000, params[\"batch_size\"]):\n",
    "        # Update metrics for this batch\n",
    "        batch_loss, batch_accuracy, gradient_norms = metrics.update_metrics()\n",
    "\n",
    "        run.log_metrics(\n",
    "            data={\n",
    "                \"metrics/train/loss\": batch_loss,\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"epoch\": epoch,\n",
    "                **gradient_norms,\n",
    "            },\n",
    "            step=step_counter,\n",
    "        )\n",
    "        step_counter += 1\n",
    "\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: _Analyze training behavior_\n",
    "While your model trains, use Neptune's web interface to monitor and analyze metrics in near real-time:\n",
    "\n",
    "**1. Real-time metric visualization**\n",
    "- Navigate to the _Charts_ tab to view live training metrics\n",
    "- Monitor multiple metrics simultaneously\n",
    "- Track training progress in real-time\n",
    "\n",
    "**2. Advanced metric filtering**\n",
    "- Focus on specific metrics using [advanced regex search](https://docs.neptune.ai/charts#filtering-charts)\n",
    "- Example: `norm & layer_\\d` filter all gradient norms layers\n",
    "- Perfect for isolating problematic layers or components\n",
    "\n",
    "**3. Create custom dashboards**\n",
    "- Save filtered metrics to a [custom dashboard](https://docs.neptune.ai/custom_dashboard) for continuous monitoring\n",
    "- Automatically updates during training\n",
    "- Share with team members\n",
    "- Ideal for tracking known problematic layers\n",
    "\n",
    "**4. Dynamic metric analysis**\n",
    "- Group related metrics (e.g., all layer gradients)\n",
    "- Create automatically updating charts by [selecting metrics dynamically](https://docs.neptune.ai/chart_widget/#dynamic-metric-selection)\n",
    "- Quickly identify vanishing/exploding gradients\n",
    "- Example query: `layer_\\d`\n",
    "\n",
    "_Example dashboard:_\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/leo/pytorch-tutorial/runs/compare?viewId=standard-view&dash=dashboard&dashboardId=9e9e6760-9e80-433a-867f-1a7d1ff40d30&compare=u4ir2KsDA_hFibTp9gV7tFm8XD-q2pARCUx-tpoKiKFc&compareChartsFilter-compound=uYOMsIHr6vYTYKmC9EoPmJB2Ry6L9tcXGcV5uA99vgRg\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "\n",
    "**5. Document training insights**\n",
    "- Create [custom reports](https://docs.neptune.ai/reports) to track training progress\n",
    "- Document anomalies and successful configurations\n",
    "- Maintain training history\n",
    "- Share insights with team members\n",
    "\n",
    "_Example report:_\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/leo/pytorch-tutorial/reports/9e79d952-272a-4a38-83e5-27df4dd225ec\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "**Additional resources:**\n",
    "- [PyTorch Layer-wise Tracking Package](TODO:Link to integration for tracking layer-wise metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
