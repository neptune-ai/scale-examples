{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Model Training with Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lb%2Fdebugging_model_training/how-to-guides/debug-model-training-runs/debug_trainng_runs.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Training large models requires careful monitoring of layer-wise metrics to catch issues early. \n",
    "\n",
    "Neptune makes it easy to track and visualize metrics like gradient norms across all layers of your model - helping you identify problems like vanishing/exploding gradients quickly.\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "1. **Initialize Neptune** and **log configuration parameters**\n",
    "2. Track **layer-wise gradient norms** during training \n",
    "3. Analyze the metrics in Neptune's UI to **debug training issues**\n",
    "\n",
    "Step through a pre-configured report [here](https://scale.neptune.ai/leo/pytorch-tutorial/reports/9e79d952-272a-4a38-83e5-27df4dd225ec) to see a finalized version.\n",
    "\n",
    "_Note: This is a code recipe that you can adapt for your own model training needs._\n",
    "\n",
    " ![Layer-wise gradient norms visualization in Neptune](tutorial-images/debugging_report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "Set your project name and API token as environment variables to log to your Neptune Scale project.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = YOUR_API_TOKEN\n",
    "# %env NEPTUNE_PROJECT = WORKSPACE_NAME/PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale torch datasets\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"embed_size\": 1000,\n",
    "    \"hidden_size\": 256,  # hidden size for the LSTM\n",
    "    \"dropout_prob\": 0.3,\n",
    "    \"num_lstm_layers\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data, model and other PyTorch required functions\n",
    "\n",
    "The `setup.py` script wraps the data and model creation for use in this tutorial. You can use your own data and model setup if required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 81926\n",
      "Validation samples: 935\n",
      "Vocabulary size: 128257\n",
      "Model created: MultilayerModel\n",
      "Optimizer: Adam\n",
      "Criterion: CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_training\n",
    "   \n",
    "# Setup complete training environment\n",
    "model, optimizer, criterion, train_dataloader, val_dataloader, vocab_size = setup_training(params, use_multilayer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug model training run with Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: _Initialize Neptune Run object_\n",
    "\n",
    "The `Run` object is used to log configuration parameters and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=pytorch-text&type=experiment\n"
     ]
    }
   ],
   "source": [
    "from neptune_scale import Run\n",
    "\n",
    "run = Run(\n",
    "    experiment_name=\"pytorch-text\", # Create a run that is the head of an experiment. This is also used for forking.\n",
    ")\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: _Log configuration parameters and tags_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See configuration parameters:\n",
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=pytorch-text&type=experiment&detailsTab=metadata\n"
     ]
    }
   ],
   "source": [
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/num_lstm_layers\": params[\"num_lstm_layers\"],\n",
    "        \"data/embed_size\": params[\"embed_size\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[\"text\", \"LLM\", \"Simple\", params[\"optimizer\"]])\n",
    "\n",
    "print(f\"See configuration parameters:\\n{run.get_experiment_url() + '&detailsTab=metadata'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: _Execute model training loop_\n",
    "\n",
    "In this training loop, we:\n",
    "1. Register backward hooks to capture gradient norms from all model layers with `register_full_backward_hook()`\n",
    "2. Track these norms during training to identify potential issues like vanishing/exploding gradients in a dictionary called `debugging_gradient_norms`\n",
    "3. Log the gradient norms to Neptune for visualization and analysis using the `log_metrics` method\n",
    "\n",
    "This approach allows you to monitor the learning dynamics across your entire model architecture in near real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View charts in near real-time:\n",
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=pytorch-text&type=experiment&detailsTab=charts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 09:18:43,794 neptune:ERROR: \n",
      "\n",
      "NeptuneSeriesStepNonIncreasing: Subsequent steps of a series must be increasing.\n",
      "\n",
      "This can be caused by either:\n",
      "- The step of a series value is smaller than the most recently logged step for this series\n",
      "- the step is exactly the same but the value is different\n",
      "\n",
      "For help, see https://docs-beta.neptune.ai/log_metrics\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids)\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# print(f\"Step {step_counter} / {len(train_dataloader)}, Loss: {loss.item()}\")\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Log global training loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 09:18:49,998 neptune:ERROR: \n",
      "\n",
      "NeptuneSeriesStepNonIncreasing: Subsequent steps of a series must be increasing.\n",
      "\n",
      "This can be caused by either:\n",
      "- The step of a series value is smaller than the most recently logged step for this series\n",
      "- the step is exactly the same but the value is different\n",
      "\n",
      "For help, see https://docs-beta.neptune.ai/log_metrics\n",
      "\n",
      "2025-04-08 09:18:50,001 neptune:ERROR: \n",
      "\n",
      "NeptuneSeriesStepNonIncreasing: Subsequent steps of a series must be increasing.\n",
      "\n",
      "This can be caused by either:\n",
      "- The step of a series value is smaller than the most recently logged step for this series\n",
      "- the step is exactly the same but the value is different\n",
      "\n",
      "For help, see https://docs-beta.neptune.ai/log_metrics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register hooks to track gradients for each layer\n",
    "def hook_fn(module, grad_input, grad_output):\n",
    "    layer_name = next(name for name, mod in model.named_modules() if mod is module)\n",
    "    if grad_input[0] is not None:  # Check if gradients exist\n",
    "        grad_norm = grad_input[0].norm().item()\n",
    "        debugging_gradient_norms[f\"debug/gradient/{layer_name}/norm\"] = grad_norm\n",
    "\n",
    "# Define dictionary of metrics to log to Neptune\n",
    "debugging_gradient_norms = {}\n",
    "# Register hooks once before training\n",
    "for name, module in model.named_modules():\n",
    "    module.register_full_backward_hook(hook_fn)\n",
    "\n",
    "# Create custom Neptune URLS for tutorial steps\n",
    "print(f\"View charts in near real-time:\\n{run.get_experiment_url() + '&detailsTab=charts'}\")\n",
    "\n",
    "step_counter = 0\n",
    "# Training loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        step_counter += 1\n",
    "        input_ids = batch[\"input_ids\"].to(params[\"device\"])\n",
    "        labels = batch[\"labels\"].to(params[\"device\"])\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log global training loss and layer-wise gradient norms\n",
    "        run.log_metrics(\n",
    "            data={\n",
    "                  \"metrics/train/loss\": loss.item(), \n",
    "                  **debugging_gradient_norms\n",
    "                  },\n",
    "            step=step_counter,\n",
    "        )\n",
    "\n",
    "# Close run to ensure all operations are processed\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: _Analyze and debug training_\n",
    "While the model is training, you can start using the Neptune web app to browse your metrics and create custom analyses and visualizations:\n",
    "1. To visualize the large number of metrics being logged in near real time, navigate to the **Charts** tab of the active run (_or select link above_).\n",
    "2. Filter the metrics using the [advanced regex searching capabilities](https://docs-beta.neptune.ai/charts#filtering-charts). For example, enter `gradient & fc & layers.[0-5] & norm` in the search bar. This query filters the metrics for the first 6 layers of the gradients norms of the fully connected layers. You can specify down to exactly the metrics name you want.\n",
    "\n",
    "![Alt text](tutorial-images/debugging_regex_search.png)\n",
    "\n",
    "3. Export the filter to a [dashboard](https://docs-beta.neptune.ai/custom_dashboard). The saved dashboard will now only display these metrics during training. This is useful if you know that a certain set of layers can be troublesome during training. \n",
    "4. Alternatively, use the [dynamic metric selection](https://docs-beta.neptune.ai/chart_widget#dynamic-metric-selection) and create a new chart widget to display all LSTM layers gradient norms in one chart. Again, use the `(.*gradient)(.*lstm)(.*norm)` query. This makes it easy to have an automatically updating chart that allows you to view all layers on a single chart for rapid debugging in case vanishing or exploding gradients appear. \n",
    "\n",
    "![Alt text](tutorial-images/debugging_dashboard.png)\n",
    "\n",
    "5. To document this behavior, create a [custom report](https://docs-beta.neptune.ai/reports) to outline the model training, global metrics, debugging metrics for the model you're training. This allows you to keep track of any anomalies but also to see what worked or did not work during training.\n",
    "\n",
    "![Alt text](tutorial-images/debugging_report.png)\n",
    "\n",
    "See the pre-configured [example of the training report](https://scale.neptune.ai/leo/pytorch-tutorial/reports/9e79d952-272a-4a38-83e5-27df4dd225ec).\n",
    "\n",
    "See also: PyTorch layer-wise tracking package [here](TODO:Link to integration for tracking layer-wise metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
