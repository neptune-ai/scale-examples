{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log and visualize debugging metrics in Neptune\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/master/how-to-guides/debug-model-training-runs/notebooks/debug_training_runs.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://github.com/neptune-ai/scale-examples/blob/main/how-to-guides/debug-model-training-runs/notebooks/debug_training_runs.ipynb\">\n",
    "  <img alt=\"Open in GitHub\" src=\"https://img.shields.io/badge/Open_in_GitHub-blue?logo=github&labelColor=black\">\n",
    "</a>\n",
    "<a target=\"_blank\" href=\"https://docs-beta.neptune.ai/tutorials/\">\n",
    "  <img alt=\"View tutorial in docs\" src=\"https://neptune.ai/wp-content/uploads/2024/01/docs-badge-2.svg\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Training large models requires careful monitoring of layer-wise metrics to catch issues early. \n",
    "\n",
    "Neptune makes it easy to track and visualize metrics like gradient norms across all layers of your model – and helps you quickly identify problems such as vanishing or exploding gradients.\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "1. **Initialize Neptune** and **log configuration parameters**.\n",
    "2. Track **layer-wise gradient norms** during training.\n",
    "3. Analyze the metrics in Neptune's UI to **debug training issues**.\n",
    "\n",
    "See a pre-configured report in the Neptune app:\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/examples/debug-training-metrics/reports/Analyze-debugging-metrics-9f0f017e-c95a-4347-8bd4-2c50120f8315\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "\n",
    "_Note: This is a code recipe that you can adapt for your own model training needs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "Set your project name and API token as environment variables to log to your Neptune Scale project.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = YOUR_API_TOKEN\n",
    "# %env NEPTUNE_PROJECT = WORKSPACE_NAME/PROJECT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries\n",
    "\n",
    "This tutorial uses PyTorch and the MNIST dataset for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale torch torchvision \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training simulation\n",
    "\n",
    "This tutorial uses a simple PyTorch model trained on the MNIST dataset to demonstrate Neptune's debugging capabilities. The training code tracks real training metrics including:\n",
    "- Loss values from the `CrossEntropyLoss` function\n",
    "- Gradient norms for each layer to monitor optimization\n",
    "\n",
    "In the next cell, we will:\n",
    "1. Import PyTorch libraries and utilities.\n",
    "2. Create a `SimpleModel` class with 20 layers.\n",
    "3. Add gradient norm tracking to identify potential issues during training.\n",
    "\n",
    "Note that this setup is not optimized for the best model, but for illustrating how to use Neptune for debugging the training runs that we create.\n",
    "\n",
    "_You can replace this simulation with your actual model training code._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 784,\n",
    "        hidden_size: int = 128,\n",
    "        output_size: int = 10,\n",
    "        num_layers: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        # Combine all layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def get_gradient_norms(self) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate the L2 norm of gradients for each layer.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing gradient norms for each layer\n",
    "        \"\"\"\n",
    "        gradient_norms = {}\n",
    "\n",
    "        # Iterate through all named parameters\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                # Calculate L2 norm of gradients\n",
    "                norm = param.grad.norm(2).item()\n",
    "                # Store in dictionary with a descriptive key\n",
    "                gradient_norms[f\"debug/L2_grad_norm/{name}\"] = norm\n",
    "\n",
    "        return gradient_norms\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "params = {\n",
    "    \"batch_size\": 512,\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.001,\n",
    "    \"num_layers\": 20,  # Configurable number of layers\n",
    "}\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleModel(num_layers=params[\"num_layers\"]).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug model training runs with Neptune\n",
    "\n",
    "In this section, we'll walk through four key steps to effectively debug your training runs:\n",
    "\n",
    "1. **Initialize Neptune** – set up the Neptune environment to track your training metrics.\n",
    "2. **Log configuration parameters** – record your model's hyperparameters and setup.\n",
    "3. **Track layer-wise metrics** – monitor gradient norms across all model layers.\n",
    "4. **Analyze training behavior** – Use Neptune's visualization tools to identify and diagnose training issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize Neptune Run object\n",
    "\n",
    "Use the `Run` object to log configuration parameters and metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune_scale import Run\n",
    "\n",
    "run = Run(\n",
    "    experiment_name=\"debugging-gradient-norms\",  # Create a run that is the head of an experiment\n",
    ")\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Log configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_configs(\n",
    "    {\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/lr\": params[\"lr\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[\"debug\", \"gradient-norm\"])\n",
    "\n",
    "print(f\"See configuration parameters:\\n{run.get_experiment_url() + '&detailsTab=metadata'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Track gradient norms during training\n",
    "\n",
    "In this training loop, we will:\n",
    "1. Calculate `loss` and `L2 gradient norms` from the `named_parameters`.\n",
    "2. Track gradient norms during training to identify potential issues like vanishing or exploding gradients in a dictionary called `gradient_norms`.\n",
    "3. Use the `log_metrics` method to log the gradient norms to Neptune for visualization and analysis.\n",
    "\n",
    "This approach allows you to monitor the learning dynamics across your entire model architecture in near real-time.\n",
    "\n",
    "_You can also use hooks to capture layer-wise training dynamics and log to Neptune_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counter = 0\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(data.size(0), -1)  # Flatten the images\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        gradient_norms = model.get_gradient_norms()\n",
    "        batch_loss = loss.item()\n",
    "\n",
    "        run.log_metrics(\n",
    "            data={\n",
    "                \"metrics/train/loss\": batch_loss,\n",
    "                \"epoch\": epoch,\n",
    "                **gradient_norms,\n",
    "            },\n",
    "            step=step_counter,\n",
    "        )\n",
    "        step_counter += 1\n",
    "\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Analyze training behavior\n",
    "While your model trains, use Neptune's web interface to monitor and analyze metrics in near real-time:\n",
    "\n",
    "**1. Real-time metric visualization**\n",
    "- To view live training, navigate to the _Charts_ tab \n",
    "- Monitor multiple metrics simultaneously\n",
    "- Track training progress in real-time\n",
    "\n",
    "**2. Advanced metric filtering**\n",
    "- Focus on specific metrics using [advanced regex search](https://docs.neptune.ai/charts#filtering-charts)\n",
    "- Example: `norm | \\d.weight` filters all gradient norms layers\n",
    "- Perfect for isolating problematic layers or components\n",
    "\n",
    "**3. Create custom dashboards**\n",
    "- Save filtered metrics to a [custom dashboard](https://docs.neptune.ai/custom_dashboard) for continuous monitoring\n",
    "- Automatically updates during training\n",
    "- You can share dashboards with team members\n",
    "- Ideal for tracking known problematic layers\n",
    "\n",
    "**4. Dynamic metric analysis**\n",
    "- Group related metric, for example all layer gradients\n",
    "- Create automatically updating charts by [selecting metrics dynamically](https://docs.neptune.ai/chart_widget/#dynamic-metric-selection)\n",
    "- Quickly identify vanishing or exploding gradients\n",
    "- Example query: `\\d.weight$`\n",
    "\n",
    "_Example dashboard:_\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/o/examples/org/debug-training-metrics/runs/compare?viewId=standard-view&dash=dashboard&dashboardId=9f0f002f-1852-448b-813d-b230de12b0b5&compare=uXbDuUl1H5I-cMeJj_n1iXy_roGF0tTL3IHVit0zU8Gs\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n",
    "\n",
    "**5. Document training insights**\n",
    "- Create [custom reports](https://docs.neptune.ai/reports) to track training progress\n",
    "- Document anomalies and successful configurations\n",
    "- Maintain training history\n",
    "- Share insights with team members\n",
    "\n",
    "_Example report:_\n",
    "<a target=\"_blank\" href=\"https://scale.neptune.ai/examples/debug-training-metrics/reports/Analyze-debugging-metrics-9f0f017e-c95a-4347-8bd4-2c50120f8315\">\n",
    "  <img alt=\"Explore in Neptune\" src=\"https://neptune.ai/wp-content/uploads/2024/01/neptune-badge.svg\">\n",
    "</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
