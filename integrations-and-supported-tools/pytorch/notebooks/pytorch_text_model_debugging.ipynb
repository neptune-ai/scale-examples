{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + PyTorch\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lr%2Fpytorch_example/integrations-and-supported-tools/pytorch/notebooks/pytorch_text_model_debugging.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Global metrics such as loss or accuracy provide a high-level performance snapshot and ensure training is on course.\n",
    "\n",
    "However, for large or foundation models, monitoring layer-wise metrics—such as gradients and activations—delivers critical insights into how each layer learns. This level of detail helps identify issues and fine-tune individual layers for better overall performance.\n",
    "The main challenge is the volume of data generated by layer-wise logging.\n",
    "\n",
    "Fortunately, Neptune is built for hyperscale tracking. It enables you to capture, organize, and analyze metrics from every layer without disrupting the training process. No matter how large is your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will show you how to:\n",
    "- Initialize the **Neptune Run** object and log configuration parameters\n",
    "- Create a **reusable class** to hook layer-wise metrics (`HookManager`)\n",
    "- Log **aggregated metrics** such as loss and accuracy\n",
    "- Log **layer-wise metrics** to debug model training such as:\n",
    "\n",
    "| **Metric**                        | **Demonstrated** | **What it shows**                                                                                             | **How to capture**                                             |\n",
    "|-----------------------------------|--------------------------------------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Activations**                   | Yes                                  | Dead or exploding activations can indicate issues with training stability. | `HookManager`            |\n",
    "| **Gradients**                     | Yes                                  | Essential for diagnosing vanishing or exploding gradients. Small gradients may indicate vanishing gradients, while large ones can signal instability. | `HookManager`        |\n",
    "| **Parameters**            | Yes                                  | Tracks how the model’s parameters evolve during training. Large or small weights may indicate the need for better regularization or adjustments in learning rate. | Extract directly from the model’s parameters.                  |\n",
    "| **Loss**               | No                                   | Identifies which parts of the network contribute more to the overall loss, aiding debugging and optimization. | Monitor outputs from each layer and compare with the target.   |\n",
    "| **Learning rate**       | No                                   | Helpful if using techniques like Layer-wise Learning Rate Decay (L2LRD). Tracking this can provide insight into the layer-specific learning rate. | Manually track based on optimizer settings.                    |\n",
    "| **Output norms**            | No                                   | The L2-norm of layer outputs can highlight issues like gradient explosion or vanishing gradients. | Compute the L2-norm for each layer’s output.                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook as a code recipe. Add your own code and adapt the sections to your own model training needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "By setting your project name and API token as environment variables, you can use them throughout this notebook.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = \"your_api_token\"\n",
    "# %env NEPTUNE_PROJECT = \"your_workspace_name_here/your_project_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "from neptune_scale import Run\n",
    "import os\n",
    "\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "\n",
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"embed_size\": 1000,\n",
    "    \"hidden_size\": 256,  # hidden size for the LSTM\n",
    "    \"dropout_prob\": 0.3,\n",
    "    \"num_lstm_layers\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download or use next token prediction dataset\n",
    "This example uses the dataset from [HuggingFace](https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset) (HF).\n",
    "\n",
    "You can increase the size of the dataset to test the logging capabilities of Neptune. Note that increasing the size will increase the time needed for the dataset to download. The current setup only downloads the first parquet file from the Hugging Face public dataset.\n",
    "\n",
    "The validation dataset is also reduced to decrease the training loop execution time. To increase the validation size, change the `test_size` key-value pair in the `train_test_split()` method from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 81926 \n",
      "Validation samples: 935\n"
     ]
    }
   ],
   "source": [
    "# For the example, download a random subset of 10% of the original dataset\n",
    "base_url = \"https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset/resolve/main/data/\"\n",
    "data_files = {\n",
    "    \"train\": base_url\n",
    "    + \"train-00001-of-00067.parquet\",  # download only the first 10 files from the HF dataset\n",
    "    \"validation\": base_url + \"validation-00000-of-00001.parquet\",\n",
    "}  # download the complete validation dataset\n",
    "\n",
    "data_subset = load_dataset(\"parquet\", data_files=data_files, num_proc=4)\n",
    "# validation_subset = load_dataset(\"parquet\", data_files = {\"validation\": base_url + \"validation-00000-of-00001.parquet\"}, num_proc=4, split=[\"validation[:5%]\"])\n",
    "validation_subset = data_subset.get(\"validation\").train_test_split(test_size=0.1)\n",
    "print(\n",
    "    f\"Training samples: {data_subset['train'].num_rows} \\nValidation samples: {validation_subset['test'].num_rows}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `DataLoader` objects\n",
    "To execute the models with PyTorch, convert the training and validation datasets to tensors. Then, set up `DataLoader` for easier batching in the training loop.\n",
    "\n",
    "The model architecture requires the vocabulary size as an input and this is why we calculate the max token from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128257\n"
     ]
    }
   ],
   "source": [
    "train_subset = data_subset[\"train\"].with_format(\n",
    "    type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    ")  # HF provides methods to convert data types to tensors\n",
    "validation_subset = validation_subset[\"test\"].with_format(\n",
    "    type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    ")  # HF provides methods to convert data types to tensors\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(validation_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Determine the vocab size of the dataset\n",
    "# Flatten the list of tokenized sentences into one long list of token IDs\n",
    "vocab_size = (\n",
    "    max([token for sentence in data_subset[\"train\"][\"input_ids\"] for token in sentence]) + 1\n",
    ")\n",
    "params[\"vocab_size\"] = vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTorch model architecture and helpers\n",
    "Define a simple LLM model architecture using PyTorch. Since this is a text-based example, we use an embedding layer, a LSTM layer, and a fully connected layer.\n",
    "\n",
    "You can adjust this architecture to your needs and increase its size when testing the workflow:\n",
    "- To increase the size of the LSTM layers, change the `num_layers` parameter in the parameters dictionary.\n",
    "- To increase the number of fully connected layers, update the mode architecture itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple LLM model with LSTM\n",
    "class SimpleLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(SimpleLLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM returns output and hidden/cell state tuple\n",
    "        out = self.fc1(lstm_out)  # Use the last output from the LSTM\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to evaluate the model after each epoch/step\n",
    "def evaluate(model, val_dataloader, criterion, device, vocab_size):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            logits = model(input_ids)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_dataloader)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tracking class\n",
    "\n",
    "This section creates a `HookManager` class:\n",
    "- It allows to capture the **activations** and **gradients** from each layer.\n",
    "- It accepts a PyTorch model object as an input.\n",
    "- It automatically captures the gradients and activations in each layer of the model.\n",
    "\n",
    "See a pseudo implementation:\n",
    "\n",
    "```python\n",
    "# Initialize model\n",
    "model = your_ModelClass()\n",
    "# Register hooks\n",
    "hm = HookManager(model)\n",
    "hm.register_hooks()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    \n",
    "    # Forward pass, e.g. model.train()\n",
    "    # Backward pass, e.g. loss.backward()\n",
    "    \n",
    "    activations = hm.get_activations()\n",
    "    gradients = hm.get_gradients()\n",
    "\n",
    "    # Log values (mean, std, etc.) to Neptune\n",
    "```\n",
    "\n",
    "_Important_: You can use the `HookManager` class in your own training script as it only accepts a model object as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to manage hooks for activations and gradients\n",
    "class HookManager:\n",
    "    def __init__(self, model):\n",
    "        if not isinstance(model, nn.Module):\n",
    "            raise TypeError(\"The model must be a PyTorch model\")\n",
    "\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    # Function to save activations\n",
    "    def save_activation(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.activations[name] = output\n",
    "\n",
    "        return hook\n",
    "\n",
    "    # Function to save gradients (registering hooks for the model parameters)\n",
    "    def save_gradient(self, name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            self.gradients[name] = grad_output[0]\n",
    "\n",
    "        return hook\n",
    "\n",
    "    # Function to register hooks for activations and gradients\n",
    "    def register_hooks(self):\n",
    "        # Register forward hooks for activations\n",
    "        for name, module in self.model.named_modules():\n",
    "            self.hooks.append(module.register_forward_hook(self.save_activation(name)))\n",
    "\n",
    "        # Register backward hooks for gradients\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, (nn.LSTM, nn.Linear)):  # You can add more layer types here\n",
    "                self.hooks.append(module.register_full_backward_hook(self.save_gradient(name)))\n",
    "\n",
    "    # Function to clear activations and gradients after use\n",
    "    def clear(self):\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    # Function to get activations\n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "    # Function to get gradients\n",
    "    def get_gradients(self):\n",
    "        return self.gradients\n",
    "\n",
    "\n",
    "from typing import Literal, List, Optional  \n",
    "class TorchWatcher:\n",
    "    def __init__(self, \n",
    "                 model: nn.Module, \n",
    "                 run: Run, \n",
    "                 ) -> None:\n",
    "        \n",
    "        self.model = model\n",
    "        self.run = run\n",
    "        self.hm = HookManager(model)\n",
    "        self.hm.register_hooks()\n",
    "        self.debug_metrics = {}\n",
    "\n",
    "    def track_activations(self): \n",
    "        # Track activations\n",
    "        activations = self.hm.get_activations()\n",
    "        for layer, activation in activations.items():\n",
    "            if layer is not None:\n",
    "                self.debug_metrics[f\"debug/activation/{layer}_mean\"] = activation[0].mean().item()\n",
    "                self.debug_metrics[f\"debug/activation/{layer}_std\"] = activation[0].std().item()\n",
    "\n",
    "    def track_gradients(self):\n",
    "        # Track gradients with hooks\n",
    "        gradients = self.hm.get_gradients()\n",
    "        for layer, gradient in gradients.items():\n",
    "            self.debug_metrics[f\"debug/gradient/{layer}_mean\"] = gradient.mean().item()\n",
    "\n",
    "    def track_parameters(self):\n",
    "        # Track gradients per layer at each epoch\n",
    "        for layer, param in self.model.named_parameters():\n",
    "            if param is not None:\n",
    "                self.debug_metrics[f\"debug/parameters/{layer}_std\"] = param.grad.std().item()\n",
    "                self.debug_metrics[f\"debug/parameters/{layer}_mean\"] = param.grad.mean().item()\n",
    "                self.debug_metrics[f\"debug/parameters/{layer}_norm\"] = (\n",
    "                    param.grad.norm().item()\n",
    "                )  # L2 norm (Euclidean norm) of the gradients\n",
    "\n",
    "    def watch(self, step: int|float, log: Optional[List[Literal[\"gradients\", \"parameters\", \"activations\"]]] | None = \"all\"):\n",
    "        match log:\n",
    "            case \"gradients\":\n",
    "                self.track_gradients()\n",
    "            case \"parameters\":\n",
    "                self.track_parameters()\n",
    "            case \"all\":\n",
    "                self.track_gradients()\n",
    "                self.track_parameters()\n",
    "\n",
    "        self.run.log_metrics(\n",
    "            data=self.debug_metrics,\n",
    "            step=step\n",
    "        )\n",
    "\n",
    "        self.hm.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neptune run object and log hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=pytorch-text&type=experiment\n"
     ]
    }
   ],
   "source": [
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "custom_run_id = f\"pytorch-text-{uuid4()}\"  # Create your own custom run_id\n",
    "experiment_name = \"pytorch-text\"  # Create a run that is the head of an experiment. This will also be used for forking.\n",
    "\n",
    "run = Run(\n",
    "    run_id=custom_run_id,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/num_lstm_layers\": params[\"num_lstm_layers\"],\n",
    "        \"data/vocab_size\": params[\"vocab_size\"],\n",
    "        \"data/embed_size\": params[\"embed_size\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"text\", \"LLM\", \"Simple\"])\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute model training loop\n",
    "In this loop, we configure the `HookManager` and register the hooks.\n",
    "\n",
    "In your training loop, use the `get_` methods to retrieve the stored values for the activations and gradients after the forward and backward passes are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 / 10241, Loss: 11.762494087219238\n",
      "1\n",
      "gradients\n",
      "Step 2 / 10241, Loss: 11.673644065856934\n",
      "2\n",
      "gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 18:14:46,982 neptune:ERROR: \n",
      "\n",
      "NeptuneSeriesStepNonIncreasing: Subsequent steps of a series must be increasing.\n",
      "\n",
      "This can be caused by either:\n",
      "- The step of a series value is smaller than the most recently logged step for this series\n",
      "- the step is exactly the same but the value is different\n",
      "\n",
      "For help, see https://docs-beta.neptune.ai/log_metrics\n",
      "\n",
      "2025-03-27 18:14:46,987 neptune:ERROR: \n",
      "\n",
      "NeptuneSeriesStepNonIncreasing: Subsequent steps of a series must be increasing.\n",
      "\n",
      "This can be caused by either:\n",
      "- The step of a series value is smaller than the most recently logged step for this series\n",
      "- the step is exactly the same but the value is different\n",
      "\n",
      "For help, see https://docs-beta.neptune.ai/log_metrics\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 / 10241, Loss: 10.27919864654541\n",
      "3\n",
      "gradients\n",
      "Step 4 / 10241, Loss: 9.545785903930664\n",
      "4\n",
      "gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 18:14:52,233 neptune:ERROR: \n",
      "\n",
      "NeptuneSeriesStepNonIncreasing: Subsequent steps of a series must be increasing.\n",
      "\n",
      "This can be caused by either:\n",
      "- The step of a series value is smaller than the most recently logged step for this series\n",
      "- the step is exactly the same but the value is different\n",
      "\n",
      "For help, see https://docs-beta.neptune.ai/log_metrics\n",
      "\n",
      "2025-03-27 18:14:52,234 neptune:ERROR: \n",
      "\n",
      "NeptuneSeriesStepNonIncreasing: Subsequent steps of a series must be increasing.\n",
      "\n",
      "This can be caused by either:\n",
      "- The step of a series value is smaller than the most recently logged step for this series\n",
      "- the step is exactly the same but the value is different\n",
      "\n",
      "For help, see https://docs-beta.neptune.ai/log_metrics\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 / 10241, Loss: 9.699625968933105\n",
      "5\n",
      "gradients\n",
      "Step 6 / 10241, Loss: 9.690692901611328\n",
      "6\n",
      "gradients\n",
      "Step 7 / 10241, Loss: 10.060093879699707\n",
      "7\n",
      "gradients\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Compute the loss (ignore padding tokens by masking labels)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m, in \u001b[0;36mSimpleLLM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     11\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)  \u001b[38;5;66;03m# LSTM returns output and hidden/cell state tuple\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the last output from the LSTM\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "debug_metrics = {}\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = SimpleLLM(\n",
    "    params[\"vocab_size\"], params[\"embed_size\"], params[\"hidden_size\"], params[\"num_lstm_layers\"]\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=-100\n",
    ")  # Ignore the buffering index of -100 in the dataset\n",
    "\n",
    "# Define watcher class\n",
    "watcher = TorchWatcher(model=model, run=run)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "step_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        step_counter += 1\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # Compute the loss (ignore padding tokens by masking labels)\n",
    "        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Step {step_counter} / {len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        # Call watch() method in loop determing which layer-wise metrics to watch\n",
    "        watcher.watch(step=step_counter, log=\"gradients\")\n",
    "\n",
    "        if step_counter % 50 == 0:  # Log validation loss at every 50 steps\n",
    "            val_loss = evaluate(model, val_dataloader, criterion, device, vocab_size)\n",
    "\n",
    "            run.log_metrics(\n",
    "                data={\n",
    "                    \"metrics/train/loss\": loss.item(),\n",
    "                    \"metrics/validation/loss\": val_loss,\n",
    "                    \"epoch/value\": epoch,\n",
    "                    **debug_metrics,\n",
    "                },\n",
    "                step=step_counter,\n",
    "            )\n",
    "        else:  # Log training loss and debugging metrics for each step\n",
    "            run.log_metrics(\n",
    "                data={\"metrics/train/loss\": loss.item(), \"epoch/value\": epoch, **debug_metrics},\n",
    "                step=step_counter,\n",
    "            )\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Close run to ensure all operations are processed\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "While the model is training, you can start using the Neptune web app to browse your metrics and create custom analyses and visualizations:\n",
    "1. To visualize the large number of metrics being logged in near real time, navigate to the **Charts** tab of the active run.\n",
    "2. Filter the metrics using the [advanced regex searching capabilities](https://docs-beta.neptune.ai/charts#filtering-charts). For example, enter `.*gradient+.*fc\\d` in the search bar. This query filters all metrics for the gradients of the fully connected layers. The more FC layers, the more charts will appear.\n",
    "3. Export the filter to a [dashboard](https://docs-beta.neptune.ai/custom_dashboard). The saved dashboard will now only display these metrics during training.\n",
    "4. Use the [dynamic metric selection](https://docs-beta.neptune.ai/chart_widget#dynamic-metric-selection) and update the chart widget to display all fully connected layers gradients in one chart. Again, use the `.*gradient+.*fc\\d` query.\n",
    "5. Create a [custom report](https://docs-beta.neptune.ai/reports) to outline the model training, global metrics, debugging metrics, and more.\n",
    "\n",
    "See also [a generic example of the training result](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
