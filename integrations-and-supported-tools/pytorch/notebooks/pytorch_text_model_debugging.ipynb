{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + PyTorch\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/neptune-ai/scale-examples/blob/lr%2Fpytorch_example/integrations-and-supported-tools/pytorch/notebooks/pytorch_text_model_debugging.ipynb\"> \n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/> \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Global metrics such as loss or accuracy provide a high-level performance snapshot and ensure training is on course.\n",
    "\n",
    "However, for large or foundation models, monitoring layer-wise metrics—such as gradients and activations—delivers critical insights into how each layer learns. This level of detail helps identify issues and fine-tune individual layers for better overall performance.\n",
    "The main challenge is the volume of data generated by layer-wise logging.\n",
    "\n",
    "Fortunately, Neptune is built for hyperscale tracking. It enables you to capture, organize, and analyze metrics from every layer without disrupting the training process. No matter how large is your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will show you how to:\n",
    "- Initialize the **Neptune Run** object and log configuration parameters\n",
    "- Create a **reusable class** to hook and log layer-wise metrics (`TorchWatcher`)\n",
    "- Log **aggregated metrics** such as loss and accuracy\n",
    "- Log **layer-wise metrics** to debug model training such as:\n",
    "\n",
    "| **Metric**                        | **Demonstrated** | **What it shows**                                                                                             | **How to capture**                                             |\n",
    "|-----------------------------------|--------------------------------------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Activations**                   | Yes                                  | Dead or exploding activations can indicate issues with training stability. | `TorchWatcher`            |\n",
    "| **Gradients**                     | Yes                                  | Essential for diagnosing vanishing or exploding gradients. Small gradients may indicate vanishing gradients, while large ones can signal instability. | `TorchWatcher`        |\n",
    "| **Parameters**            | Yes                                  | Tracks how the model’s parameters evolve during training. Large or small weights may indicate the need for better regularization or adjustments in learning rate. | Extract directly from the model’s parameters.                  |\n",
    "| **Loss**               | No                                   | Identifies which parts of the network contribute more to the overall loss, aiding debugging and optimization. | Monitor outputs from each layer and compare with the target.   |\n",
    "| **Learning rate**       | No                                   | Helpful if using techniques like Layer-wise Learning Rate Decay (L2LRD). Tracking this can provide insight into the layer-specific learning rate. | Manually track based on optimizer settings.                    |\n",
    "| **Output norms**            | No                                   | The L2-norm of layer outputs can highlight issues like gradient explosion or vanishing gradients. | Compute the L2-norm for each layer’s output.                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook as a code recipe. Add your own code and adapt the sections to your own model training needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables\n",
    "By setting your project name and API token as environment variables, you can use them throughout this notebook.\n",
    "\n",
    "Uncomment the code block below and replace placeholder values with your own credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Neptune credentials as environment variables\n",
    "# %env NEPTUNE_API_TOKEN = \"your_api_token\"\n",
    "# %env NEPTUNE_PROJECT = \"your_workspace_name_here/your_project_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -qU neptune_scale torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "from neptune_scale import Run\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "\n",
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"embed_size\": 1000,\n",
    "    \"hidden_size\": 256,  # hidden size for the LSTM\n",
    "    \"dropout_prob\": 0.3,\n",
    "    \"num_lstm_layers\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download or use next token prediction dataset\n",
    "This example uses the dataset from [HuggingFace](https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset) (HF).\n",
    "\n",
    "You can increase the size of the dataset to test the logging capabilities of Neptune. Note that increasing the size will increase the time needed for the dataset to download. The current setup only downloads the first parquet file from the Hugging Face public dataset.\n",
    "\n",
    "The validation dataset is also reduced to decrease the training loop execution time. To increase the validation size, change the `test_size` key-value pair in the `train_test_split()` method from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 81926 \n",
      "Validation samples: 935\n"
     ]
    }
   ],
   "source": [
    "# For the example, download a random subset of 10% of the original dataset\n",
    "base_url = \"https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset/resolve/main/data/\"\n",
    "data_files = {\n",
    "    \"train\": base_url\n",
    "    + \"train-00001-of-00067.parquet\",  # download only the first 10 files from the HF dataset\n",
    "    \"validation\": base_url + \"validation-00000-of-00001.parquet\",\n",
    "}  # download the complete validation dataset\n",
    "\n",
    "data_subset = load_dataset(\"parquet\", data_files=data_files, num_proc=4)\n",
    "# validation_subset = load_dataset(\"parquet\", data_files = {\"validation\": base_url + \"validation-00000-of-00001.parquet\"}, num_proc=4, split=[\"validation[:5%]\"])\n",
    "validation_subset = data_subset.get(\"validation\").train_test_split(test_size=0.1)\n",
    "print(\n",
    "    f\"Training samples: {data_subset['train'].num_rows} \\nValidation samples: {validation_subset['test'].num_rows}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `DataLoader` objects\n",
    "To execute the models with PyTorch, convert the training and validation datasets to tensors. Then, set up `DataLoader` for easier batching in the training loop.\n",
    "\n",
    "The model architecture requires the vocabulary size as an input and this is why we calculate the max token from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128257\n"
     ]
    }
   ],
   "source": [
    "train_subset = data_subset[\"train\"].with_format(\n",
    "    type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    ")  # HF provides methods to convert data types to tensors\n",
    "validation_subset = validation_subset[\"test\"].with_format(\n",
    "    type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]\n",
    ")  # HF provides methods to convert data types to tensors\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(validation_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Determine the vocab size of the dataset\n",
    "# Flatten the list of tokenized sentences into one long list of token IDs\n",
    "vocab_size = (\n",
    "    max([token for sentence in data_subset[\"train\"][\"input_ids\"] for token in sentence]) + 1\n",
    ")\n",
    "params[\"vocab_size\"] = vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTorch model architecture and helpers\n",
    "Define a simple LLM model architecture using PyTorch. Since this is a text-based example, we use an embedding layer, a LSTM layer, and a fully connected layer.\n",
    "\n",
    "You can adjust this architecture to your needs and increase its size when testing the workflow:\n",
    "- To increase the size of the LSTM layers, change the `num_layers` parameter in the parameters dictionary.\n",
    "- To increase the number of fully connected layers, update the mode architecture itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple LLM model with LSTM\n",
    "class SimpleLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(SimpleLLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM returns output and hidden/cell state tuple\n",
    "        out = self.fc1(lstm_out)  # Use the last output from the LSTM\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to evaluate the model after each epoch/step\n",
    "def evaluate(model, val_dataloader, criterion, device, vocab_size):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            logits = model(input_ids)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_dataloader)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup tracking class\n",
    "\n",
    "This section intializes the `TorchWatcher` class:\n",
    "- It accepts a PyTorch model object as an input. \n",
    "- It allows you to capture the **parameters**, **activations** and **gradients** from each layer.\n",
    "- Specify which tensor statistics to capture.\n",
    "\n",
    "See a pseudo implementation:\n",
    "\n",
    "```python\n",
    "from TorchWatcher import TorchWatcher\n",
    "\n",
    "model = YourModel()\n",
    "watcher = TorchWatcher(model, run)  # Uses default mean() statistic\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    # Forward pass\n",
    "    output = model(x_batch)\n",
    "    loss = criterion(output, y_batch)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track metrics after the forward and backward passes\n",
    "    watcher.watch(step=epoch)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TorchWatcher import TorchWatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neptune run object and log hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scale.neptune.ai/leo/pytorch-tutorial/runs/details?runIdentificationKey=pytorch-text&type=experiment\n"
     ]
    }
   ],
   "source": [
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "custom_run_id = f\"pytorch-text-{uuid4()}\"  # Create your own custom run_id\n",
    "experiment_name = \"pytorch-text\"  # Create a run that is the head of an experiment. This will also be used for forking.\n",
    "\n",
    "run = Run(\n",
    "    run_id=custom_run_id,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/num_lstm_layers\": params[\"num_lstm_layers\"],\n",
    "        \"data/vocab_size\": params[\"vocab_size\"],\n",
    "        \"data/embed_size\": params[\"embed_size\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"text\", \"LLM\", \"Simple\"])\n",
    "\n",
    "print(run.get_experiment_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute model training loop\n",
    "\n",
    "In the training loop we call the `watch()` method from the `TorchWatcher` package after the backward and forward passes to track our parameters, gradients and activations with a combination of tensor statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 / 10241, Loss: 11.765020370483398\n",
      "Step 2 / 10241, Loss: 11.628341674804688\n",
      "Step 3 / 10241, Loss: 9.686809539794922\n",
      "Step 4 / 10241, Loss: 9.657011032104492\n",
      "Step 5 / 10241, Loss: 9.74715518951416\n",
      "Step 6 / 10241, Loss: 10.450525283813477\n",
      "Step 7 / 10241, Loss: 10.163161277770996\n",
      "Step 8 / 10241, Loss: 10.278069496154785\n",
      "Step 9 / 10241, Loss: 10.855117797851562\n",
      "Step 10 / 10241, Loss: 10.982325553894043\n",
      "Step 11 / 10241, Loss: 10.332056999206543\n",
      "Step 12 / 10241, Loss: 10.331168174743652\n",
      "Step 13 / 10241, Loss: 10.291738510131836\n",
      "Step 14 / 10241, Loss: 9.950544357299805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\autograd\\function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model and optimizer\n",
    "model = SimpleLLM(\n",
    "    params[\"vocab_size\"], params[\"embed_size\"], params[\"hidden_size\"], params[\"num_lstm_layers\"]\n",
    ")\n",
    "optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=-100\n",
    ")  # Ignore the buffering index of -100 in the dataset\n",
    "\n",
    "# Define watcher class\n",
    "watcher = TorchWatcher(model=model, run=run, tensor_stats=[\"mean\", \"norm\"], base_namespace=\"debug_metrics\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "step_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        step_counter += 1\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # Compute the loss (ignore padding tokens by masking labels)\n",
    "        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Step {step_counter} / {len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        # Call watch() method in loop determing which layer-wise metrics to watch\n",
    "        watcher.watch(step=step_counter, track_activations=False, track_parameters=False)\n",
    "\n",
    "        if step_counter % 50 == 0:  # Log validation loss at every 50 steps\n",
    "            val_loss = evaluate(model, val_dataloader, criterion, device, vocab_size)\n",
    "\n",
    "            run.log_metrics(\n",
    "                data={\n",
    "                    \"metrics/train/loss\": loss.item(),\n",
    "                    \"metrics/validation/loss\": val_loss,\n",
    "                    \"epoch/value\": epoch,\n",
    "                },\n",
    "                step=step_counter,\n",
    "            )\n",
    "        else:  # Log training loss and debugging metrics for each step\n",
    "            run.log_metrics(\n",
    "                data={\"metrics/train/loss\": loss.item(), \"epoch/value\": epoch},\n",
    "                step=step_counter,\n",
    "            )\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# Close run to ensure all operations are processed\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "While the model is training, you can start using the Neptune web app to browse your metrics and create custom analyses and visualizations:\n",
    "1. To visualize the large number of metrics being logged in near real time, navigate to the **Charts** tab of the active run.\n",
    "2. Filter the metrics using the [advanced regex searching capabilities](https://docs-beta.neptune.ai/charts#filtering-charts). For example, enter `.*gradient+.*fc\\d` in the search bar. This query filters all metrics for the gradients of the fully connected layers. The more FC layers, the more charts will appear.\n",
    "3. Export the filter to a [dashboard](https://docs-beta.neptune.ai/custom_dashboard). The saved dashboard will now only display these metrics during training.\n",
    "4. Use the [dynamic metric selection](https://docs-beta.neptune.ai/chart_widget#dynamic-metric-selection) and update the chart widget to display all fully connected layers gradients in one chart. Again, use the `.*gradient+.*fc\\d` query.\n",
    "5. Create a [custom report](https://docs-beta.neptune.ai/reports) to outline the model training, global metrics, debugging metrics, and more.\n",
    "\n",
    "See also [a generic example of the training result](https://scale.neptune.ai/o/examples/org/LLM-Pretraining/reports/9e6a2cad-77e7-42df-9d64-28f07d37e908).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
