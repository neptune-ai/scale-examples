{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + Pytorch\n",
    "## Logging and visualizing debugging metrics in PyTorch\n",
    "\n",
    "Introduction\n",
    "\n",
    "See how Neptune Scale can be used for foundation model traning when you are required to track a large number of metrics across your transformers architecture.\n",
    "\n",
    "This guide will show you how to:\n",
    "- Initialize the Neptune Run object and log configuration parameters\n",
    "- Log standard loss and accuracy metrics to Neptune\n",
    "- Log debugging metrics during model training such as;\n",
    "    * Activations per layer\n",
    "    * Gradients (mean and std) per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project that you will use for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Neptune and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q -U neptune_scale torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update config to include model architecture\n",
    "# TODO - Add more hyperparameters\n",
    "# TODO - look at CNN layers\n",
    "# TODO - output and log the model architecture\n",
    "# TODO - check loss and accuracy calculations\n",
    "# TODO - clean up the evaluation function to exclude tracking gradients\n",
    "# TODO - do not use group tags\n",
    "# TODO - track the input features\n",
    "# TODO - clean the training loop of commented out code that is unused\n",
    "# TODO - add batchnormalization and drop out layers to improve the model\n",
    "# TODO - add HookManager class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 512,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_size\": 28 * 28\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform the data for training\n",
    "In this example, we will be using the MINST dataset as part of the PyTorch library for illustration. We create a train, validation and test dataset and apply a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform to normalize the data and convert it to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing the image to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # Use test set as validation\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple Convolutional Neural Network model for MNIST\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input channels = 1 (grayscale images)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Flattened size of image after convolution layers\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes for digits 0-9\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling layer to downsample\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers (increase number of layers)\n",
    "        self.fc1 = nn.Linear(params[\"input_size\"], params[\"input_features\"])\n",
    "        self.fc2 = nn.Linear(params[\"input_features\"], 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, params[\"n_classes\"])      # Output layer (10 classes for MNIST)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, params[\"input_size\"])  # Flatten the input image (28x28)\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc3(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc4(x))  # Apply ReLU activation\n",
    "        x = self.fc5(x)  # Output layer\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model (validation/test) with gradients tracked\n",
    "def evaluate(model, data_loader, criterion, device, track_gradients=False):\n",
    "    model.train() if track_gradients else model.eval()  # Ensure model is in training mode if tracking gradients\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient tracking during evaluation\n",
    "        for data, target in data_loader:\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass (with gradient tracking if specified)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)  # Correct loss computation\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if track_gradients:\n",
    "                # Track gradients (we will backpropagate but do not update model parameters)\n",
    "                loss.backward()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_preds += target.size(0)\n",
    "            correct_preds += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = correct_preds / total_preds\n",
    "    return epoch_loss / len(data_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzn8qbItEKQw"
   },
   "source": [
    "## Neptune - Initialize Training Run and Log Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neptune parameters\n",
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "run = Run(\n",
    "    project = \"leo/pytorch-tutorial\",\n",
    "    run_id=f\"pytorch-{uuid4()}\"\n",
    "    )\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/input_size\": params[\"input_size\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"Torch-MINST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Log Metrics while Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup distributed environment\n",
    "def setup(rank, world_size, backend):\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def clean_up():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def setupModel(rank, params):\n",
    "    # Instantiate the model, loss function, and optimizer\n",
    "    # model = SimpleCNN()\n",
    "    model = SimpleNN()\n",
    "    device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # Select an optimizer\n",
    "    if params[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "        print(params[\"optimizer\"])\n",
    "    elif params[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"])\n",
    "        print(params[\"optimizer\"])\n",
    "    else:\n",
    "        print(\"No optimizer selected\")\n",
    "\n",
    "    return model, optimizer, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank: int, world_size: int, params, train_dataset):\n",
    "\n",
    "    try:\n",
    "        setup(rank, world_size, \"gloo\")\n",
    "        model, optimizer, device = setupModel(rank, params)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], sampler=sampler)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = params[\"epochs\"]\n",
    "        step_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "            # Training step\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                step_counter += 1\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total_preds += target.size(0)\n",
    "                correct_preds += (predicted == target).sum().item()\n",
    "\n",
    "                batch_accuracy = correct_preds / total_preds\n",
    "\n",
    "                # Validation step per training step\n",
    "                val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)  # Evaluate after each step\n",
    "\n",
    "                if rank == 0:\n",
    "                    print(f\"Train loss: {loss.item()}\")\n",
    "                    print(f\"Accuracy: {batch_accuracy}\")\n",
    "                    print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "                dist.barrier() # synchonize processes before moving to next step\n",
    "\n",
    "            clean_up() # Clean up processes from DDP training\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training process (Rank {rank}): {e}\")\n",
    "\n",
    "        ''''\n",
    "        run.log_metrics(\n",
    "            data = {\n",
    "                \"metrics/train/loss\": loss.item(),\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"metrics/validation/loss\": val_loss,\n",
    "                \"metrics/validation/accuracy\": val_accuracy,\n",
    "                \"epoch_value\": epoch\n",
    "            },\n",
    "            step = step_counter\n",
    "        )\n",
    "\n",
    "# Final Testing Step with gradient tracking\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, track_gradients=False)  # Track gradients during test\n",
    "print(f\"Testing complete. Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "run.log_configs(\n",
    "        {\n",
    "        \"metrics/test/loss\": test_loss,\n",
    "        \"metrics/test/accuracy\": test_accuracy\n",
    "    }\n",
    ")'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neptune:INFO: Waiting for all operations to be processed\n",
      "neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "hzNRK8BNEKQz",
    "outputId": "f6c3dfe9-5942-47c9-b1a4-3d239a831713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training process (Rank 0): use_libuv was requested but PyTorch was build without libuv support\n"
     ]
    }
   ],
   "source": [
    "# clean_up()\n",
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "train(0, 1, params, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "k68YVMssEKQ0",
    "outputId": "3e159880-cd97-4a3d-9cc3-b8f853916351"
   },
   "outputs": [],
   "source": [
    "# Run DDP\n",
    "# clean_up()\n",
    "num_gpu = torch.cuda.device_count()\n",
    "mp.set_start_method('spawn', force=True)\n",
    "mp.spawn(train, args=(num_gpu, params, train_dataset), nprocs=num_gpu, join=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
