{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + PyTorch\n",
    "\n",
    "Introduction\n",
    "\n",
    "See how Neptune Scale can be used for foundation model traning when you are required to track a large number of metrics across your transformers architecture. \n",
    "\n",
    "This guide will show you how to:\n",
    "- Initialize the Neptune Run object and log configuration parameters\n",
    "- Log standard loss and accuracy metrics to Neptune\n",
    "- Log debugging metrics during model training such as;\n",
    "    * Activations per layer\n",
    "    * Gradients (mean and std weights and biases) per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project that you will use for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Neptune and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q -U neptune_scale torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update config to include model architecture\n",
    "# TODO - Add more hyperparameters\n",
    "# TODO - look at CNN layers\n",
    "# TODO - output and log the model architecture\n",
    "# TODO - check loss and accuracy calculations\n",
    "# TODO - clean up the evaluation function to exclude tracking gradients\n",
    "# TODO - do not use group tags\n",
    "# TODO - track the input features\n",
    "# TODO - clean the training loop of commented out code that is unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 256,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5, \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_size\": 28 * 28\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform the data for training\n",
    "In this example, we will be using the MINST dataset as part of the PyTorch library for illustration. We create a train, validation and test dataset and apply a transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform to normalize the data and convert it to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing the image to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # Use test set as validation\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple Convolutional Neural Network model for MNIST\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input channels = 1 (grayscale images)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Flattened size of image after convolution layers\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes for digits 0-9\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling layer to downsample\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers (increase number of layers)\n",
    "        self.fc1 = nn.Linear(params[\"input_size\"], params[\"input_features\"]) \n",
    "        self.fc2 = nn.Linear(params[\"input_features\"], 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, params[\"n_classes\"])      # Output layer (10 classes for MNIST)\n",
    "\n",
    "        # Registering hooks to track activations\n",
    "        self.hooks = []\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc1\")))\n",
    "        self.hooks.append(self.fc2.register_forward_hook(self.save_activation(\"fc2\")))\n",
    "        self.hooks.append(self.fc3.register_forward_hook(self.save_activation(\"fc3\")))\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc4\")))\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc5\")))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, params[\"input_size\"])  # Flatten the input image (28x28)\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc3(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc4(x))  # Apply ReLU activation\n",
    "        x = self.fc5(x)  # Output layer\n",
    "        return x\n",
    "    \n",
    "        # Function to save activations\n",
    "    def save_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activations[name] = output\n",
    "        return hook\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "    def clear_activations(self):\n",
    "        self.activations = {}\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "#model = SimpleCNN()\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "\n",
    "# Select an optimizer\n",
    "if params[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "elif params[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "else:\n",
    "    print(\"No optimizer selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model (validation/test) with gradients tracked\n",
    "def evaluate(model, data_loader, track_gradients=False):\n",
    "    model.train() if track_gradients else model.eval()  # Ensure model is in training mode if tracking gradients\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient tracking during evaluation\n",
    "        for data, target in data_loader:\n",
    "            # Forward pass (with gradient tracking if specified)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)  # Correct loss computation\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if track_gradients:\n",
    "                # Track gradients (we will backpropagate but do not update model parameters)\n",
    "                loss.backward()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_preds += target.size(0)\n",
    "            correct_preds += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = correct_preds / total_preds\n",
    "    return epoch_loss / len(data_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Initialize Training Run and Log Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neptune parameters\n",
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "run = Run(\n",
    "    project = \"leo/pytorch-tutorial\",\n",
    "    run_id=f\"pytorch-{uuid4()}\"\n",
    "    )\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/input_size\": params[\"input_size\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"Torch-MINST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Log Metrics while Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Training complete. Loss: 0.9066, Accuracy: 0.69%\n",
      "{'layers/layer_fc1/activation_mean': -27.44853973388672, 'layers/layer_fc4/activation_mean': -27.44853973388672, 'layers/layer_fc5/activation_mean': -27.44853973388672, 'layers/layer_fc2/activation_mean': -3.266420602798462, 'layers/layer_fc3/activation_mean': -3.977348804473877, 'layers/layer_fc1/activation_std': 9.884493827819824, 'layers/layer_fc4/activation_std': 9.884493827819824, 'layers/layer_fc5/activation_std': 9.884493827819824, 'layers/layer_fc2/activation_std': 2.8974952697753906, 'layers/layer_fc3/activation_std': 3.9147353172302246, 'layers/layer_fc1.weight_mean': 1.759609585860744e-05, 'layers/layer_fc1.bias_mean': -2.6617166440701112e-05, 'layers/layer_fc2.weight_mean': 1.845465158112347e-05, 'layers/layer_fc2.bias_mean': 6.048093564459123e-05, 'layers/layer_fc3.weight_mean': 6.554154424520675e-06, 'layers/layer_fc3.bias_mean': 4.6133769501466304e-05, 'layers/layer_fc4.weight_mean': -1.4562309843313415e-05, 'layers/layer_fc4.bias_mean': 1.2838396287406795e-05, 'layers/layer_fc5.weight_mean': 4.2628095497931895e-10, 'layers/layer_fc5.bias_mean': -9.313225884932663e-11, 'layers/layer_fc1.weight_std': 0.0014361083740368485, 'layers/layer_fc1.bias_std': 0.0014619147405028343, 'layers/layer_fc2.weight_std': 0.001974125625565648, 'layers/layer_fc2.bias_std': 0.0011026636930182576, 'layers/layer_fc3.weight_std': 0.0008759713382460177, 'layers/layer_fc3.bias_std': 0.0011844916734844446, 'layers/layer_fc4.weight_std': 0.002118356991559267, 'layers/layer_fc4.bias_std': 0.00178423966281116, 'layers/layer_fc5.weight_std': 0.021144278347492218, 'layers/layer_fc5.bias_std': 0.016225792467594147, 'grad_norm/fc1.weight': 0.6434243321418762, 'grad_norm/fc1.bias': 0.023348789662122726, 'grad_norm/fc2.weight': 0.714738130569458, 'grad_norm/fc2.bias': 0.02496359497308731, 'grad_norm/fc3.weight': 0.31714311242103577, 'grad_norm/fc3.bias': 0.018929213285446167, 'grad_norm/fc4.weight': 0.38346678018569946, 'grad_norm/fc4.bias': 0.02010788396000862, 'grad_norm/fc5.weight': 0.7561851143836975, 'grad_norm/fc5.bias': 0.04867737740278244}\n",
      "Epoch [2/5] Training complete. Loss: 0.2738, Accuracy: 0.92%\n",
      "{'layers/layer_fc1/activation_mean': -27.444847106933594, 'layers/layer_fc4/activation_mean': -27.444847106933594, 'layers/layer_fc5/activation_mean': -27.444847106933594, 'layers/layer_fc2/activation_mean': -4.043460845947266, 'layers/layer_fc3/activation_mean': -4.557342529296875, 'layers/layer_fc1/activation_std': 9.927286148071289, 'layers/layer_fc4/activation_std': 9.927286148071289, 'layers/layer_fc5/activation_std': 9.927286148071289, 'layers/layer_fc2/activation_std': 3.500694990158081, 'layers/layer_fc3/activation_std': 4.5128021240234375, 'layers/layer_fc1.weight_mean': 3.658090008684667e-07, 'layers/layer_fc1.bias_mean': 3.3925662137335166e-06, 'layers/layer_fc2.weight_mean': -2.423007288143708e-07, 'layers/layer_fc2.bias_mean': 1.9860681277350523e-05, 'layers/layer_fc3.weight_mean': 5.834312105434947e-06, 'layers/layer_fc3.bias_mean': -6.391452188836411e-06, 'layers/layer_fc4.weight_mean': 7.309272405109368e-06, 'layers/layer_fc4.bias_mean': 3.118724634987302e-06, 'layers/layer_fc5.weight_mean': 3.8910458188823327e-10, 'layers/layer_fc5.bias_mean': 1.3038515822572094e-09, 'layers/layer_fc1.weight_std': 0.0011528775794431567, 'layers/layer_fc1.bias_std': 0.0012104393681511283, 'layers/layer_fc2.weight_std': 0.0022255387157201767, 'layers/layer_fc2.bias_std': 0.0010018462780863047, 'layers/layer_fc3.weight_std': 0.0007322281016968191, 'layers/layer_fc3.bias_std': 0.0008695347351022065, 'layers/layer_fc4.weight_std': 0.0019773344974964857, 'layers/layer_fc4.bias_std': 0.0012858008267357945, 'layers/layer_fc5.weight_std': 0.015919363126158714, 'layers/layer_fc5.bias_std': 0.011463840492069721, 'grad_norm/fc1.weight': 0.5164875984191895, 'grad_norm/fc1.bias': 0.019329242408275604, 'grad_norm/fc2.weight': 0.8057278990745544, 'grad_norm/fc2.bias': 0.02265150472521782, 'grad_norm/fc3.weight': 0.26510223746299744, 'grad_norm/fc3.bias': 0.013885731808841228, 'grad_norm/fc4.weight': 0.3579327464103699, 'grad_norm/fc4.bias': 0.014490283094346523, 'grad_norm/fc5.weight': 0.5693259239196777, 'grad_norm/fc5.bias': 0.03439152240753174}\n",
      "Epoch [3/5] Training complete. Loss: 0.2052, Accuracy: 0.94%\n",
      "{'layers/layer_fc1/activation_mean': -27.409822463989258, 'layers/layer_fc4/activation_mean': -27.409822463989258, 'layers/layer_fc5/activation_mean': -27.409822463989258, 'layers/layer_fc2/activation_mean': -4.827888011932373, 'layers/layer_fc3/activation_mean': -5.646310329437256, 'layers/layer_fc1/activation_std': 10.087625503540039, 'layers/layer_fc4/activation_std': 10.087625503540039, 'layers/layer_fc5/activation_std': 10.087625503540039, 'layers/layer_fc2/activation_std': 4.256272792816162, 'layers/layer_fc3/activation_std': 5.8036675453186035, 'layers/layer_fc1.weight_mean': -1.7095830116886646e-05, 'layers/layer_fc1.bias_mean': 2.0519555619102903e-05, 'layers/layer_fc2.weight_mean': -1.0371921234764159e-05, 'layers/layer_fc2.bias_mean': -2.6764431822812185e-05, 'layers/layer_fc3.weight_mean': 1.1725012427632464e-07, 'layers/layer_fc3.bias_mean': -3.2187872420763597e-06, 'layers/layer_fc4.weight_mean': -4.269544206181308e-06, 'layers/layer_fc4.bias_mean': 2.4562015823903494e-05, 'layers/layer_fc5.weight_mean': 5.096634225765229e-10, 'layers/layer_fc5.bias_mean': 1.3038515822572094e-09, 'layers/layer_fc1.weight_std': 0.0010182075202465057, 'layers/layer_fc1.bias_std': 0.0011107329046353698, 'layers/layer_fc2.weight_std': 0.0014369278214871883, 'layers/layer_fc2.bias_std': 0.0006925289635546505, 'layers/layer_fc3.weight_std': 0.0004932511947117746, 'layers/layer_fc3.bias_std': 0.0006599067128263414, 'layers/layer_fc4.weight_std': 0.0011504496214911342, 'layers/layer_fc4.bias_std': 0.001063313102349639, 'layers/layer_fc5.weight_std': 0.010835869237780571, 'layers/layer_fc5.bias_std': 0.010039512068033218, 'grad_norm/fc1.weight': 0.4562200903892517, 'grad_norm/fc1.bias': 0.017740018665790558, 'grad_norm/fc2.weight': 0.520235002040863, 'grad_norm/fc2.bias': 0.01566654071211815, 'grad_norm/fc3.weight': 0.1785753071308136, 'grad_norm/fc3.bias': 0.010537991300225258, 'grad_norm/fc4.weight': 0.20825187861919403, 'grad_norm/fc4.bias': 0.011986151337623596, 'grad_norm/fc5.weight': 0.3875243663787842, 'grad_norm/fc5.bias': 0.030118538066744804}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m batch_accuracy \u001b[38;5;241m=\u001b[39m correct_preds \u001b[38;5;241m/\u001b[39m total_preds\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.2f}%\")\u001b[39;00m\n\u001b[0;32m     45\u001b[0m         \n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Validation step per training step\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Evaluate after each step\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# print(f\"Validation at step [{batch_idx+1}/{len(train_loader)}] - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "Cell \u001b[1;32mIn[80], line 8\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, track_gradients)\u001b[0m\n\u001b[0;32m      6\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient tracking during evaluation\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Forward pass (with gradient tracking if specified)\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Correct loss computation\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "activation_dict_mean = {}\n",
    "activation_dict_std = {}\n",
    "params_dict_std = {}\n",
    "params_dict_mean = {}\n",
    "grad_norms = {}\n",
    "\n",
    "num_epochs = params[\"epochs\"]\n",
    "step_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Reset activations for each epoch\n",
    "    model.clear_activations()\n",
    "    \n",
    "    # Training step\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        step_counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_preds += target.size(0)\n",
    "        correct_preds += (predicted == target).sum().item()\n",
    "        \n",
    "        # Print loss and accuracy for each batch (step)\n",
    "        #if (batch_idx + 1) % 5 == 0:  # Every 5 steps\n",
    "        batch_accuracy = correct_preds / total_preds\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.2f}%\")\n",
    "                \n",
    "        # Validation step per training step\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader)  # Evaluate after each step\n",
    "        # print(f\"Validation at step [{batch_idx+1}/{len(train_loader)}] - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param is not None:\n",
    "                grad_norms[f\"grad_norm/{name}\"] = param.grad.norm(2).item() # L2 norm (Euclidean norm) of the gradients\n",
    "\n",
    "        run.log_metrics(\n",
    "            data = {\n",
    "                \"metrics/train/loss\": loss.item(),\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"metrics/validation/loss\": val_loss,\n",
    "                \"metrics/validation/accuracy\": val_accuracy,\n",
    "                \"epoch_value\": epoch,\n",
    "                **grad_norms\n",
    "            },\n",
    "            step = step_counter\n",
    "        )\n",
    "    \n",
    "    # Print loss and accuracy for the entire training epoch\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training complete. Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Track activations\n",
    "    for name, activation in model.get_activations().items():\n",
    "        activation_dict_mean[f\"layers/layer_{name}/activation_mean\"] = activation.mean().item()\n",
    "        activation_dict_std[f\"layers/layer_{name}/activation_std\"] = activation.std().item()\n",
    "\n",
    "    # Track gradients and norms per layer at each epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        if param is not None:\n",
    "            params_dict_std[f\"layers/layer_{name}_std\"] = param.grad.std().item()\n",
    "            params_dict_mean[f\"layers/layer_{name}_mean\"] = param.grad.mean().item()\n",
    "            # grad_norms[f\"grad_norm/{name}\"] = param.grad.norm(2).item() # L2 norm (Euclidean norm) of the gradients\n",
    "    \n",
    "    layers_dict = {**activation_dict_mean, \n",
    "                   **activation_dict_std,\n",
    "                   **params_dict_mean,\n",
    "                   **params_dict_std,\n",
    "                   **grad_norms\n",
    "                   }\n",
    "    print(layers_dict)\n",
    "\n",
    "    # data_to_log = {\n",
    "    #        \"metrics/test/loss_epoch\": epoch_loss / len(train_loader),\n",
    "     #       \"metrics/train/accuracy_epoch\": train_accuracy\n",
    "      #  }.update(activation_dict)\n",
    "    \n",
    "    run.log_metrics(\n",
    "    data = layers_dict,\n",
    "    step = epoch\n",
    "    )\n",
    "    \n",
    "# Final Testing Step with gradient tracking\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, track_gradients=False)  # Track gradients during test\n",
    "print(f\"Testing complete. Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "run.log_configs(\n",
    "        {\n",
    "        \"metrics/test/loss\": test_loss,\n",
    "        \"metrics/test/accuracy\": test_accuracy\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neptune:INFO: Waiting for all operations to be processed\n",
      "neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "run.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
