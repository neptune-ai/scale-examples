{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + PyTorch\n",
    "\n",
    "Introduction\n",
    "\n",
    "See how Neptune Scale can be used for foundation model traning when you are required to track a large number of metrics across your transformers architecture. \n",
    "\n",
    "This guide will show you how to:\n",
    "- Initialize the Neptune Run object and log configuration parameters\n",
    "- Log standard loss and accuracy metrics to Neptune\n",
    "- Log debugging metrics during model training such as;\n",
    "    * Activations per layer\n",
    "    * Gradients (mean and std weights and biases) per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project that you will use for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Neptune and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q -U neptune_scale torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update config to include model architecture\n",
    "# TODO - Add more hyperparameters\n",
    "# TODO - Add additional logging metrics (weights, gradients, activations, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 256,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"epochs\": 5, \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_size\": 28 * 28\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform the data for training\n",
    "In this example, we will be using the MINST dataset as part of the PyTorch library for illustration. We create a train, validation and test dataset and apply a transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform to normalize the data and convert it to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing the image to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # Use test set as validation\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple Convolutional Neural Network model for MNIST\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input channels = 1 (grayscale images)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Flattened size of image after convolution layers\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes for digits 0-9\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling layer to downsample\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers (increase number of layers)\n",
    "        self.fc1 = nn.Linear(params[\"input_size\"], params[\"input_features\"]) \n",
    "        self.fc2 = nn.Linear(params[\"input_features\"], 64)\n",
    "        self.fc3 = nn.Linear(64, params[\"n_classes\"])      # Output layer (10 classes for MNIST)\n",
    "\n",
    "        # Registering hooks to track activations\n",
    "        self.hooks = []\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc1\")))\n",
    "        self.hooks.append(self.fc2.register_forward_hook(self.save_activation(\"fc2\")))\n",
    "        self.hooks.append(self.fc3.register_forward_hook(self.save_activation(\"fc3\")))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, params[\"input_size\"])  # Flatten the input image (28x28)\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "    \n",
    "        # Function to save activations\n",
    "    def save_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activations[name] = output\n",
    "        return hook\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "    def clear_activations(self):\n",
    "        self.activations = {}\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "# model = SimpleCNN()\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "\n",
    "# Select an optimizer\n",
    "if params[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "elif params[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "else:\n",
    "    print(\"No optimizer selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model (validation/test) with gradients tracked\n",
    "def evaluate(model, data_loader, track_gradients=False):\n",
    "    model.train() if track_gradients else model.eval()  # Ensure model is in training mode if tracking gradients\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient tracking during evaluation\n",
    "        for data, target in data_loader:\n",
    "            # Forward pass (with gradient tracking if specified)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)  # Correct loss computation\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if track_gradients:\n",
    "                # Track gradients (we will backpropagate but do not update model parameters)\n",
    "                loss.backward()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_preds += target.size(0)\n",
    "            correct_preds += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_preds / total_preds\n",
    "    return epoch_loss / len(data_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Initialize Training Run and Log Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neptune parameters\n",
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "run = Run(\n",
    "    project = \"leo/pytorch-tutorial\",\n",
    "    run_id=f\"pytorch-{uuid4()}\"\n",
    "    )\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"Torch-MINST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Log Metrics while Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Training complete. Loss: 2.3020, Accuracy: 10.98%\n",
      "{'layers/layer_fc1/activation_mean': -154.4237060546875, 'layers/layer_fc2/activation_mean': -0.2948070466518402, 'layers/layer_fc3/activation_mean': 0.1060420349240303, 'layers/layer_fc1/activation_std': 38.38949966430664, 'layers/layer_fc2/activation_std': 0.1372535228729248, 'layers/layer_fc3/activation_std': 0.05648726597428322, 'layers/layer_fc1.weight_mean': 0.0, 'layers/layer_fc1.bias_mean': 0.0, 'layers/layer_fc2.weight_mean': 0.0, 'layers/layer_fc2.bias_mean': 0.0, 'layers/layer_fc3.weight_mean': 0.0, 'layers/layer_fc3.bias_mean': 5.867332397713199e-09, 'layers/layer_fc1.weight_std': 0.0, 'layers/layer_fc1.bias_std': 0.0, 'layers/layer_fc2.weight_std': 0.0, 'layers/layer_fc2.bias_std': 0.0, 'layers/layer_fc3.weight_std': 0.0, 'layers/layer_fc3.bias_std': 0.031105784699320793, 'fc3.bias': 0.09331735223531723}\n",
      "Epoch [2/5] Training complete. Loss: 2.3023, Accuracy: 10.89%\n",
      "{'layers/layer_fc1/activation_mean': -154.4237060546875, 'layers/layer_fc2/activation_mean': -0.2948070466518402, 'layers/layer_fc3/activation_mean': 0.10444235801696777, 'layers/layer_fc1/activation_std': 38.38949966430664, 'layers/layer_fc2/activation_std': 0.1372535228729248, 'layers/layer_fc3/activation_std': 0.06586454063653946, 'layers/layer_fc1.weight_mean': 0.0, 'layers/layer_fc1.bias_mean': 0.0, 'layers/layer_fc2.weight_mean': 0.0, 'layers/layer_fc2.bias_mean': 0.0, 'layers/layer_fc3.weight_mean': 0.0, 'layers/layer_fc3.bias_mean': 2.2351742678949904e-09, 'layers/layer_fc1.weight_std': 0.0, 'layers/layer_fc1.bias_std': 0.0, 'layers/layer_fc2.weight_std': 0.0, 'layers/layer_fc2.bias_std': 0.0, 'layers/layer_fc3.weight_std': 0.0, 'layers/layer_fc3.bias_std': 0.028597360476851463, 'fc3.bias': 0.08579207956790924}\n",
      "Epoch [3/5] Training complete. Loss: 2.3024, Accuracy: 11.08%\n",
      "{'layers/layer_fc1/activation_mean': -154.4237060546875, 'layers/layer_fc2/activation_mean': -0.2948070466518402, 'layers/layer_fc3/activation_mean': 0.10336797684431076, 'layers/layer_fc1/activation_std': 38.38949966430664, 'layers/layer_fc2/activation_std': 0.1372535228729248, 'layers/layer_fc3/activation_std': 0.07147954404354095, 'layers/layer_fc1.weight_mean': 0.0, 'layers/layer_fc1.bias_mean': 0.0, 'layers/layer_fc2.weight_mean': 0.0, 'layers/layer_fc2.bias_mean': 0.0, 'layers/layer_fc3.weight_mean': 0.0, 'layers/layer_fc3.bias_mean': 4.842877210364804e-09, 'layers/layer_fc1.weight_std': 0.0, 'layers/layer_fc1.bias_std': 0.0, 'layers/layer_fc2.weight_std': 0.0, 'layers/layer_fc2.bias_std': 0.0, 'layers/layer_fc3.weight_std': 0.0, 'layers/layer_fc3.bias_std': 0.034048620611429214, 'fc3.bias': 0.10214585065841675}\n",
      "Epoch [4/5] Training complete. Loss: 2.3023, Accuracy: 11.11%\n",
      "{'layers/layer_fc1/activation_mean': -154.4237060546875, 'layers/layer_fc2/activation_mean': -0.2948070466518402, 'layers/layer_fc3/activation_mean': 0.10569452494382858, 'layers/layer_fc1/activation_std': 38.38949966430664, 'layers/layer_fc2/activation_std': 0.1372535228729248, 'layers/layer_fc3/activation_std': 0.08372259140014648, 'layers/layer_fc1.weight_mean': 0.0, 'layers/layer_fc1.bias_mean': 0.0, 'layers/layer_fc2.weight_mean': 0.0, 'layers/layer_fc2.bias_mean': 0.0, 'layers/layer_fc3.weight_mean': 0.0, 'layers/layer_fc3.bias_mean': 2.607703164514419e-09, 'layers/layer_fc1.weight_std': 0.0, 'layers/layer_fc1.bias_std': 0.0, 'layers/layer_fc2.weight_std': 0.0, 'layers/layer_fc2.bias_std': 0.0, 'layers/layer_fc3.weight_std': 0.0, 'layers/layer_fc3.bias_std': 0.038734495639801025, 'fc3.bias': 0.11620348691940308}\n",
      "Epoch [5/5] Training complete. Loss: 2.3026, Accuracy: 11.02%\n",
      "{'layers/layer_fc1/activation_mean': -154.4237060546875, 'layers/layer_fc2/activation_mean': -0.2948070466518402, 'layers/layer_fc3/activation_mean': 0.10467425733804703, 'layers/layer_fc1/activation_std': 38.38949966430664, 'layers/layer_fc2/activation_std': 0.1372535228729248, 'layers/layer_fc3/activation_std': 0.0657070130109787, 'layers/layer_fc1.weight_mean': 0.0, 'layers/layer_fc1.bias_mean': 0.0, 'layers/layer_fc2.weight_mean': 0.0, 'layers/layer_fc2.bias_mean': 0.0, 'layers/layer_fc3.weight_mean': 0.0, 'layers/layer_fc3.bias_mean': 1.1175870895385742e-08, 'layers/layer_fc1.weight_std': 0.0, 'layers/layer_fc1.bias_std': 0.0, 'layers/layer_fc2.weight_std': 0.0, 'layers/layer_fc2.bias_std': 0.0, 'layers/layer_fc3.weight_std': 0.0, 'layers/layer_fc3.bias_std': 0.03915564715862274, 'fc3.bias': 0.11746694147586823}\n",
      "Testing complete. Loss: 2.3026, Accuracy: 10.32%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "activation_dict_mean = {}\n",
    "activation_dict_std = {}\n",
    "params_dict_std = {}\n",
    "params_dict_mean = {}\n",
    "grad_norms = {}\n",
    "\n",
    "num_epochs = params[\"epochs\"]\n",
    "step_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Reset activations for each epoch\n",
    "    model.clear_activations()\n",
    "    \n",
    "    # Training step\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        step_counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_preds += target.size(0)\n",
    "        correct_preds += (predicted == target).sum().item()\n",
    "        \n",
    "        # Print loss and accuracy for each batch (step)\n",
    "        #if (batch_idx + 1) % 5 == 0:  # Every 5 steps\n",
    "        batch_accuracy = 100 * correct_preds / total_preds\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.2f}%\")\n",
    "                \n",
    "        # Validation step per training step\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader)  # Evaluate after each step\n",
    "        # print(f\"Validation at step [{batch_idx+1}/{len(train_loader)}] - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        run.log_metrics(\n",
    "            data = {\n",
    "                \"metrics/train/loss\": loss.item(),\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"metrics/validation/loss\": val_loss,\n",
    "                \"metrics/validation/accuracy\": val_accuracy,\n",
    "                \"epoch_value\": epoch\n",
    "            },\n",
    "            step = step_counter\n",
    "        )\n",
    "    \n",
    "    # Print loss and accuracy for the entire training epoch\n",
    "    train_accuracy = 100 * correct_preds / total_preds\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training complete. Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Track activations\n",
    "    for name, activation in model.get_activations().items():\n",
    "        activation_dict_mean[f\"layers/layer_{name}/activation_mean\"] = activation.mean().item()\n",
    "        activation_dict_std[f\"layers/layer_{name}/activation_std\"] = activation.std().item()\n",
    "\n",
    "    # Track gradients and norms per layer at each epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        params_dict_std[f\"layers/layer_{name}_std\"] = param.grad.std().item()\n",
    "        params_dict_mean[f\"layers/layer_{name}_mean\"] = param.grad.mean().item()\n",
    "        grad_norms[f\"grad_norm/{name}\"] = param.grad.norm(2).item() # L2 norm (Euclidean norm) of the gradients\n",
    "    \n",
    "    layers_dict = {**activation_dict_mean, \n",
    "                   **activation_dict_std,\n",
    "                   **params_dict_mean,\n",
    "                   **params_dict_std,\n",
    "                   **grad_norms\n",
    "                   }\n",
    "    print(layers_dict)\n",
    "\n",
    "    # data_to_log = {\n",
    "    #        \"metrics/test/loss_epoch\": epoch_loss / len(train_loader),\n",
    "     #       \"metrics/train/accuracy_epoch\": train_accuracy\n",
    "      #  }.update(activation_dict)\n",
    "    \n",
    "    run.log_metrics(\n",
    "    data = layers_dict,\n",
    "    step = epoch\n",
    "    )\n",
    "    \n",
    "# Final Testing Step with gradient tracking\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, track_gradients=False)  # Track gradients during test\n",
    "print(f\"Testing complete. Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "run.log_configs(\n",
    "        {\n",
    "        \"metrics/test/loss\": test_loss,\n",
    "        \"metrics/test/accuracy\": test_accuracy\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neptune:INFO: Waiting for all operations to be processed\n",
      "neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "run.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
