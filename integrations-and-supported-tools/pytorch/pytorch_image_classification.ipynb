{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + Pytorch\n",
    "## Logging and visualizing debugging metrics in PyTorch\n",
    "\n",
    "Introduction\n",
    "\n",
    "See how Neptune Scale can be used for foundation model traning when you are required to track a large number of metrics across your transformers architecture. \n",
    "\n",
    "This guide will show you how to:\n",
    "- Initialize the Neptune Run object and log configuration parameters\n",
    "- Log standard loss and accuracy metrics to Neptune\n",
    "- Log debugging metrics during model training such as;\n",
    "    * Activations per layer\n",
    "    * Gradients (mean and std) per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project that you will use for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Neptune and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q -U neptune_scale torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update config to include model architecture\n",
    "# TODO - Add more hyperparameters\n",
    "# TODO - look at CNN layers\n",
    "# TODO - output and log the model architecture\n",
    "# TODO - check loss and accuracy calculations\n",
    "# TODO - clean up the evaluation function to exclude tracking gradients\n",
    "# TODO - do not use group tags\n",
    "# TODO - track the input features\n",
    "# TODO - clean the training loop of commented out code that is unused\n",
    "# TODO - add batchnormalization and drop out layers to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 256,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5, \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_size\": 28 * 28\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform the data for training\n",
    "In this example, we will be using the MINST dataset as part of the PyTorch library for illustration. We create a train, validation and test dataset and apply a transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform to normalize the data and convert it to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing the image to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # Use test set as validation\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple Convolutional Neural Network model for MNIST\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input channels = 1 (grayscale images)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Flattened size of image after convolution layers\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes for digits 0-9\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling layer to downsample\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers (increase number of layers)\n",
    "        self.fc1 = nn.Linear(params[\"input_size\"], params[\"input_features\"]) \n",
    "        self.fc2 = nn.Linear(params[\"input_features\"], 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, params[\"n_classes\"])      # Output layer (10 classes for MNIST)\n",
    "\n",
    "        # Registering hooks to track activations\n",
    "        self.hooks = []\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc1\")))\n",
    "        self.hooks.append(self.fc2.register_forward_hook(self.save_activation(\"fc2\")))\n",
    "        self.hooks.append(self.fc3.register_forward_hook(self.save_activation(\"fc3\")))\n",
    "        self.hooks.append(self.fc4.register_forward_hook(self.save_activation(\"fc4\")))\n",
    "        self.hooks.append(self.fc5.register_forward_hook(self.save_activation(\"fc5\")))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, params[\"input_size\"])  # Flatten the input image (28x28)\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc3(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc4(x))  # Apply ReLU activation\n",
    "        x = self.fc5(x)  # Output layer\n",
    "        return x\n",
    "    \n",
    "        # Function to save activations\n",
    "    def save_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activations[name] = output\n",
    "        return hook\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "    def clear_activations(self):\n",
    "        self.activations = {}\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "#model = SimpleCNN()\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "\n",
    "# Select an optimizer\n",
    "if params[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "elif params[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "else:\n",
    "    print(\"No optimizer selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model (validation/test) with gradients tracked\n",
    "def evaluate(model, data_loader, track_gradients=False):\n",
    "    model.train() if track_gradients else model.eval()  # Ensure model is in training mode if tracking gradients\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient tracking during evaluation\n",
    "        for data, target in data_loader:\n",
    "            # Forward pass (with gradient tracking if specified)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)  # Correct loss computation\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if track_gradients:\n",
    "                # Track gradients (we will backpropagate but do not update model parameters)\n",
    "                loss.backward()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_preds += target.size(0)\n",
    "            correct_preds += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = correct_preds / total_preds\n",
    "    return epoch_loss / len(data_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Initialize Training Run and Log Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neptune parameters\n",
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "run = Run(\n",
    "    project = \"leo/pytorch-tutorial\",\n",
    "    run_id=f\"pytorch-{uuid4()}\"\n",
    "    )\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/input_size\": params[\"input_size\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"Torch-MINST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Log Metrics while Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Training complete. Loss: 0.5882, Accuracy: 0.80%\n",
      "{'layers/layer_fc1/activation_mean': -28.098386764526367, 'layers/layer_fc4/activation_mean': -28.098386764526367, 'layers/layer_fc5/activation_mean': -28.098386764526367, 'layers/layer_fc2/activation_mean': -4.294741630554199, 'layers/layer_fc3/activation_mean': -4.590151309967041, 'layers/layer_fc1/activation_std': 10.138233184814453, 'layers/layer_fc4/activation_std': 10.138233184814453, 'layers/layer_fc5/activation_std': 10.138233184814453, 'layers/layer_fc2/activation_std': 3.513108730316162, 'layers/layer_fc3/activation_std': 4.694570064544678, 'layers/layer_fc1.weight_mean': -4.2270323319826275e-05, 'layers/layer_fc1.bias_mean': 5.756489190389402e-05, 'layers/layer_fc2.weight_mean': 1.7486901924712583e-05, 'layers/layer_fc2.bias_mean': 5.119257184560411e-05, 'layers/layer_fc3.weight_mean': -6.444352038670331e-06, 'layers/layer_fc3.bias_mean': -3.2650757930241525e-05, 'layers/layer_fc4.weight_mean': 4.735463880933821e-05, 'layers/layer_fc4.bias_mean': 0.00019672312191687524, 'layers/layer_fc5.weight_mean': 2.773958840407431e-11, 'layers/layer_fc5.bias_mean': -3.4924596548080444e-10, 'layers/layer_fc1.weight_std': 0.0018884788732975721, 'layers/layer_fc1.bias_std': 0.0020714548882097006, 'layers/layer_fc2.weight_std': 0.002078161109238863, 'layers/layer_fc2.bias_std': 0.0017285202629864216, 'layers/layer_fc3.weight_std': 0.0006235535256564617, 'layers/layer_fc3.bias_std': 0.0014959467807784677, 'layers/layer_fc4.weight_std': 0.001397876301780343, 'layers/layer_fc4.bias_std': 0.001897347392514348, 'layers/layer_fc5.weight_std': 0.014698885381221771, 'layers/layer_fc5.bias_std': 0.015608142130076885, 'grad_norm/fc1.weight': 0.8462492823600769, 'grad_norm/fc1.bias': 0.03309129923582077, 'grad_norm/fc2.weight': 0.7523983716964722, 'grad_norm/fc2.bias': 0.039090901613235474, 'grad_norm/fc3.weight': 0.22576160728931427, 'grad_norm/fc3.bias': 0.023894065991044044, 'grad_norm/fc4.weight': 0.25318393111228943, 'grad_norm/fc4.bias': 0.021497542038559914, 'grad_norm/fc5.weight': 0.5256778597831726, 'grad_norm/fc5.bias': 0.04682442545890808}\n",
      "Epoch [2/5] Training complete. Loss: 0.2444, Accuracy: 0.93%\n",
      "{'layers/layer_fc1/activation_mean': -28.045276641845703, 'layers/layer_fc4/activation_mean': -28.045276641845703, 'layers/layer_fc5/activation_mean': -28.045276641845703, 'layers/layer_fc2/activation_mean': -6.288389682769775, 'layers/layer_fc3/activation_mean': -7.880246162414551, 'layers/layer_fc1/activation_std': 10.427655220031738, 'layers/layer_fc4/activation_std': 10.427655220031738, 'layers/layer_fc5/activation_std': 10.427655220031738, 'layers/layer_fc2/activation_std': 5.016049385070801, 'layers/layer_fc3/activation_std': 7.8910441398620605, 'layers/layer_fc1.weight_mean': -1.8336819266551174e-05, 'layers/layer_fc1.bias_mean': 2.4514503820682876e-05, 'layers/layer_fc2.weight_mean': -4.397415978019126e-06, 'layers/layer_fc2.bias_mean': -5.701838745153509e-06, 'layers/layer_fc3.weight_mean': 3.609377472457709e-06, 'layers/layer_fc3.bias_mean': 2.2656186047242954e-05, 'layers/layer_fc4.weight_mean': 3.2792174806672847e-06, 'layers/layer_fc4.bias_mean': 6.183851655805483e-05, 'layers/layer_fc5.weight_mean': 1.033322427623773e-09, 'layers/layer_fc5.bias_mean': 2.793967834868738e-10, 'layers/layer_fc1.weight_std': 0.0009652073495090008, 'layers/layer_fc1.bias_std': 0.0011003392282873392, 'layers/layer_fc2.weight_std': 0.0015912855742499232, 'layers/layer_fc2.bias_std': 0.0008093378273770213, 'layers/layer_fc3.weight_std': 0.000562622444704175, 'layers/layer_fc3.bias_std': 0.0008122650324366987, 'layers/layer_fc4.weight_std': 0.0013753673993051052, 'layers/layer_fc4.bias_std': 0.001243805163539946, 'layers/layer_fc5.weight_std': 0.02044137567281723, 'layers/layer_fc5.bias_std': 0.01635204255580902, 'grad_norm/fc1.weight': 0.4324897825717926, 'grad_norm/fc1.bias': 0.01757538504898548, 'grad_norm/fc2.weight': 0.57610684633255, 'grad_norm/fc2.bias': 0.018295787274837494, 'grad_norm/fc3.weight': 0.2036944478750229, 'grad_norm/fc3.bias': 0.012975896708667278, 'grad_norm/fc4.weight': 0.24896498024463654, 'grad_norm/fc4.bias': 0.014034420251846313, 'grad_norm/fc5.weight': 0.7310471534729004, 'grad_norm/fc5.bias': 0.04905613139271736}\n",
      "Epoch [3/5] Training complete. Loss: 0.2124, Accuracy: 0.94%\n",
      "{'layers/layer_fc1/activation_mean': -28.00644874572754, 'layers/layer_fc4/activation_mean': -28.00644874572754, 'layers/layer_fc5/activation_mean': -28.00644874572754, 'layers/layer_fc2/activation_mean': -8.306803703308105, 'layers/layer_fc3/activation_mean': -10.63320255279541, 'layers/layer_fc1/activation_std': 10.678731918334961, 'layers/layer_fc4/activation_std': 10.678731918334961, 'layers/layer_fc5/activation_std': 10.678731918334961, 'layers/layer_fc2/activation_std': 6.999919891357422, 'layers/layer_fc3/activation_std': 10.323572158813477, 'layers/layer_fc1.weight_mean': 7.430891855619848e-05, 'layers/layer_fc1.bias_mean': -9.825517918216065e-05, 'layers/layer_fc2.weight_mean': 1.702215740806423e-05, 'layers/layer_fc2.bias_mean': 5.076182424090803e-05, 'layers/layer_fc3.weight_mean': 2.006397380682756e-06, 'layers/layer_fc3.bias_mean': 9.8647487902781e-06, 'layers/layer_fc4.weight_mean': 3.732190089067444e-05, 'layers/layer_fc4.bias_mean': 0.00015728663129266351, 'layers/layer_fc5.weight_mean': 5.190377017072478e-10, 'layers/layer_fc5.bias_mean': 7.450580707946131e-10, 'layers/layer_fc1.weight_std': 0.0013835423160344362, 'layers/layer_fc1.bias_std': 0.0014023504918441176, 'layers/layer_fc2.weight_std': 0.0018697652267292142, 'layers/layer_fc2.bias_std': 0.0008941664709709585, 'layers/layer_fc3.weight_std': 0.0007453133002854884, 'layers/layer_fc3.bias_std': 0.0008750202250666916, 'layers/layer_fc4.weight_std': 0.0016579279908910394, 'layers/layer_fc4.bias_std': 0.001322976779192686, 'layers/layer_fc5.weight_std': 0.018788360059261322, 'layers/layer_fc5.bias_std': 0.0166013166308403, 'grad_norm/fc1.weight': 0.620717465877533, 'grad_norm/fc1.bias': 0.02244885452091694, 'grad_norm/fc2.weight': 0.6769528388977051, 'grad_norm/fc2.bias': 0.020245518535375595, 'grad_norm/fc3.weight': 0.26983216404914856, 'grad_norm/fc3.bias': 0.013973843306303024, 'grad_norm/fc4.weight': 0.30018848180770874, 'grad_norm/fc4.bias': 0.015015012584626675, 'grad_norm/fc5.weight': 0.671930193901062, 'grad_norm/fc5.bias': 0.049803949892520905}\n",
      "Epoch [4/5] Training complete. Loss: 0.1774, Accuracy: 0.95%\n",
      "{'layers/layer_fc1/activation_mean': -28.02176284790039, 'layers/layer_fc4/activation_mean': -28.02176284790039, 'layers/layer_fc5/activation_mean': -28.02176284790039, 'layers/layer_fc2/activation_mean': -9.155248641967773, 'layers/layer_fc3/activation_mean': -13.8362455368042, 'layers/layer_fc1/activation_std': 10.731369972229004, 'layers/layer_fc4/activation_std': 10.731369972229004, 'layers/layer_fc5/activation_std': 10.731369972229004, 'layers/layer_fc2/activation_std': 7.8137617111206055, 'layers/layer_fc3/activation_std': 12.559845924377441, 'layers/layer_fc1.weight_mean': 4.3527179514057934e-05, 'layers/layer_fc1.bias_mean': -5.993247759761289e-05, 'layers/layer_fc2.weight_mean': -8.3655349953915e-06, 'layers/layer_fc2.bias_mean': -2.7970250812359154e-05, 'layers/layer_fc3.weight_mean': -6.794801265641581e-06, 'layers/layer_fc3.bias_mean': -5.020684693590738e-05, 'layers/layer_fc4.weight_mean': -5.56191080249846e-05, 'layers/layer_fc4.bias_mean': -0.00022704749426338822, 'layers/layer_fc5.weight_mean': -4.933287667263642e-10, 'layers/layer_fc5.bias_mean': -1.0710209386033398e-09, 'layers/layer_fc1.weight_std': 0.0006279172375798225, 'layers/layer_fc1.bias_std': 0.0006310796015895903, 'layers/layer_fc2.weight_std': 0.0009842520812526345, 'layers/layer_fc2.bias_std': 0.00036153788096271455, 'layers/layer_fc3.weight_std': 0.0003291342291049659, 'layers/layer_fc3.bias_std': 0.0003552739799488336, 'layers/layer_fc4.weight_std': 0.0006492410320788622, 'layers/layer_fc4.bias_std': 0.0005454771453514695, 'layers/layer_fc5.weight_std': 0.0076590655371546745, 'layers/layer_fc5.bias_std': 0.005646508652716875, 'grad_norm/fc1.weight': 0.28198111057281494, 'grad_norm/fc1.bias': 0.010123053565621376, 'grad_norm/fc2.weight': 0.35634881258010864, 'grad_norm/fc2.bias': 0.008197144605219364, 'grad_norm/fc3.weight': 0.119184210896492, 'grad_norm/fc3.bias': 0.005729861091822386, 'grad_norm/fc4.weight': 0.11795385181903839, 'grad_norm/fc4.bias': 0.006662336643785238, 'grad_norm/fc5.weight': 0.2739119827747345, 'grad_norm/fc5.bias': 0.016939526423811913}\n",
      "Epoch [5/5] Training complete. Loss: 0.1836, Accuracy: 0.95%\n",
      "{'layers/layer_fc1/activation_mean': -27.910594940185547, 'layers/layer_fc4/activation_mean': -27.910594940185547, 'layers/layer_fc5/activation_mean': -27.910594940185547, 'layers/layer_fc2/activation_mean': -11.856008529663086, 'layers/layer_fc3/activation_mean': -16.673297882080078, 'layers/layer_fc1/activation_std': 11.18954086303711, 'layers/layer_fc4/activation_std': 11.18954086303711, 'layers/layer_fc5/activation_std': 11.18954086303711, 'layers/layer_fc2/activation_std': 10.051806449890137, 'layers/layer_fc3/activation_std': 14.125566482543945, 'layers/layer_fc1.weight_mean': -4.3949068640358746e-05, 'layers/layer_fc1.bias_mean': 5.6177024816861376e-05, 'layers/layer_fc2.weight_mean': -3.007975647051353e-05, 'layers/layer_fc2.bias_mean': -6.0376849432941526e-05, 'layers/layer_fc3.weight_mean': -6.851441867183894e-06, 'layers/layer_fc3.bias_mean': -4.150162567384541e-05, 'layers/layer_fc4.weight_mean': -3.618385744630359e-05, 'layers/layer_fc4.bias_mean': -9.973671694751829e-05, 'layers/layer_fc5.weight_mean': 1.3166733259240004e-09, 'layers/layer_fc5.bias_mean': 1.3737008197622913e-09, 'layers/layer_fc1.weight_std': 0.0014514160575345159, 'layers/layer_fc1.bias_std': 0.001587417908012867, 'layers/layer_fc2.weight_std': 0.002324556466192007, 'layers/layer_fc2.bias_std': 0.000764612341299653, 'layers/layer_fc3.weight_std': 0.0007226017769426107, 'layers/layer_fc3.bias_std': 0.0007500615902245045, 'layers/layer_fc4.weight_std': 0.001422140165232122, 'layers/layer_fc4.bias_std': 0.000992378918454051, 'layers/layer_fc5.weight_std': 0.016280390322208405, 'layers/layer_fc5.bias_std': 0.013839689083397388, 'grad_norm/fc1.weight': 0.6505305767059326, 'grad_norm/fc1.bias': 0.025364963337779045, 'grad_norm/fc2.weight': 0.8416466116905212, 'grad_norm/fc2.bias': 0.017338205128908157, 'grad_norm/fc3.weight': 0.26162052154541016, 'grad_norm/fc3.bias': 0.01199591625481844, 'grad_norm/fc4.weight': 0.25751423835754395, 'grad_norm/fc4.bias': 0.011240324936807156, 'grad_norm/fc5.weight': 0.5822373628616333, 'grad_norm/fc5.bias': 0.04151906818151474}\n",
      "Testing complete. Loss: 0.1949, Accuracy: 0.95%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "activation_dict_mean = {}\n",
    "activation_dict_std = {}\n",
    "params_dict_std = {}\n",
    "params_dict_mean = {}\n",
    "grad_norms = {}\n",
    "\n",
    "num_epochs = params[\"epochs\"]\n",
    "step_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Reset activations for each epoch\n",
    "    model.clear_activations()\n",
    "    \n",
    "    # Training step\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        step_counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_preds += target.size(0)\n",
    "        correct_preds += (predicted == target).sum().item()\n",
    "        \n",
    "        # Print loss and accuracy for each batch (step)\n",
    "        #if (batch_idx + 1) % 5 == 0:  # Every 5 steps\n",
    "        batch_accuracy = correct_preds / total_preds\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.2f}%\")\n",
    "                \n",
    "        # Validation step per training step\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader)  # Evaluate after each step\n",
    "        # print(f\"Validation at step [{batch_idx+1}/{len(train_loader)}] - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param is not None:\n",
    "                grad_norms[f\"grad_norm/{name}\"] = param.grad.norm(2).item() # L2 norm (Euclidean norm) of the gradients\n",
    "\n",
    "        run.log_metrics(\n",
    "            data = {\n",
    "                \"metrics/train/loss\": loss.item(),\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"metrics/validation/loss\": val_loss,\n",
    "                \"metrics/validation/accuracy\": val_accuracy,\n",
    "                \"epoch_value\": epoch,\n",
    "                **grad_norms\n",
    "            },\n",
    "            step = step_counter\n",
    "        )\n",
    "    \n",
    "    # Print loss and accuracy for the entire training epoch\n",
    "    train_accuracy = correct_preds / total_preds\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training complete. Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Track activations\n",
    "    for name, activation in model.get_activations().items():\n",
    "        activation_dict_mean[f\"layers/layer_{name}/activation_mean\"] = activation.mean().item()\n",
    "        activation_dict_std[f\"layers/layer_{name}/activation_std\"] = activation.std().item()\n",
    "\n",
    "    # Track gradients and norms per layer at each epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        if param is not None:\n",
    "            params_dict_std[f\"layers/layer_{name}_std\"] = param.grad.std().item()\n",
    "            params_dict_mean[f\"layers/layer_{name}_mean\"] = param.grad.mean().item()\n",
    "            # grad_norms[f\"grad_norm/{name}\"] = param.grad.norm(2).item() # L2 norm (Euclidean norm) of the gradients\n",
    "    \n",
    "    layers_dict = {**activation_dict_mean, \n",
    "                   **activation_dict_std,\n",
    "                   **params_dict_mean,\n",
    "                   **params_dict_std,\n",
    "                   **grad_norms\n",
    "                   }\n",
    "    print(layers_dict)\n",
    "\n",
    "    # data_to_log = {\n",
    "    #        \"metrics/test/loss_epoch\": epoch_loss / len(train_loader),\n",
    "     #       \"metrics/train/accuracy_epoch\": train_accuracy\n",
    "      #  }.update(activation_dict)\n",
    "    \n",
    "    run.log_metrics(\n",
    "    data = layers_dict,\n",
    "    step = epoch\n",
    "    )\n",
    "    \n",
    "# Final Testing Step with gradient tracking\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, track_gradients=False)  # Track gradients during test\n",
    "print(f\"Testing complete. Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "run.log_configs(\n",
    "        {\n",
    "        \"metrics/test/loss\": test_loss,\n",
    "        \"metrics/test/accuracy\": test_accuracy\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neptune:INFO: Waiting for all operations to be processed\n",
      "neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "run.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
