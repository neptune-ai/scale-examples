{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + PyTorch\n",
    "\n",
    "Introduction\n",
    "\n",
    "See how Neptune Scale can be used for foundation model traning when you are required to track a large number of metrics across your transformers architecture. \n",
    "\n",
    "This guide will show you how to:\n",
    "- Initialize the Neptune Run object and log configuration parameters\n",
    "- Log standard loss and accuracy metrics to Neptune\n",
    "- Log debugging metrics during model training such as;\n",
    "    * Activations per layer\n",
    "    * Gradients (mean and std weights and biases) per layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project that you will use for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Neptune and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q -U neptune_scale torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - update config to include model architecture\n",
    "# TODO - Add more hyperparameters\n",
    "# TODO - Add additional logging metrics (weights, gradients, activations, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 256,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"epochs\": 5, \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"n_classes\": 10,\n",
    "    \"input_size\": 28 * 28\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and transform the data for training\n",
    "In this example, we will be using the MINST dataset as part of the PyTorch library for illustration. We create a train, validation and test dataset and apply a transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform to normalize the data and convert it to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizing the image to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # Use test set as validation\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple Convolutional Neural Network model for MNIST\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # Input channels = 1 (grayscale images)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Flattened size of image after convolution layers\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes for digits 0-9\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Pooling layer to downsample\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers (increase number of layers)\n",
    "        self.fc1 = nn.Linear(params[\"input_size\"], params[\"input_features\"]) \n",
    "        self.fc2 = nn.Linear(params[\"input_features\"], 150)\n",
    "        self.fc3 = nn.Linear(150, 100)\n",
    "        self.fc4 = nn.Linear(100, 64)\n",
    "        self.fc5 = nn.Linear(64, params[\"n_classes\"])      # Output layer (10 classes for MNIST)\n",
    "\n",
    "        # Registering hooks to track activations\n",
    "        self.hooks = []\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc1\")))\n",
    "        self.hooks.append(self.fc2.register_forward_hook(self.save_activation(\"fc2\")))\n",
    "        self.hooks.append(self.fc3.register_forward_hook(self.save_activation(\"fc3\")))\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc4\")))\n",
    "        self.hooks.append(self.fc1.register_forward_hook(self.save_activation(\"fc5\")))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, params[\"input_size\"])  # Flatten the input image (28x28)\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc2(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc3(x))  # Apply ReLU activation\n",
    "        x = torch.relu(self.fc4(x))  # Apply ReLU activation\n",
    "        x = self.fc5(x)  # Output layer\n",
    "        return x\n",
    "    \n",
    "        # Function to save activations\n",
    "    def save_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            self.activations[name] = output\n",
    "        return hook\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "    def clear_activations(self):\n",
    "        self.activations = {}\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "# model = SimpleCNN()\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "\n",
    "# Select an optimizer\n",
    "if params[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "elif params[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params[\"learning_rate\"])\n",
    "    print(params[\"optimizer\"])\n",
    "else:\n",
    "    print(\"No optimizer selected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model (validation/test) with gradients tracked\n",
    "def evaluate(model, data_loader, track_gradients=False):\n",
    "    model.train() if track_gradients else model.eval()  # Ensure model is in training mode if tracking gradients\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient tracking during evaluation\n",
    "        for data, target in data_loader:\n",
    "            # Forward pass (with gradient tracking if specified)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)  # Correct loss computation\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if track_gradients:\n",
    "                # Track gradients (we will backpropagate but do not update model parameters)\n",
    "                loss.backward()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_preds += target.size(0)\n",
    "            correct_preds += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_preds / total_preds\n",
    "    return epoch_loss / len(data_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Initialize Training Run and Log Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neptune parameters\n",
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "run = Run(\n",
    "    project = \"leo/pytorch-tutorial\",\n",
    "    run_id=f\"pytorch-{uuid4()}\"\n",
    "    )\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"Torch-MINST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune - Log Metrics while Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Training complete. Loss: 2.4092, Accuracy: 37.12%\n",
      "{'layers/layer_fc1/activation_mean': -162.4604949951172, 'layers/layer_fc4/activation_mean': -162.4604949951172, 'layers/layer_fc5/activation_mean': -162.4604949951172, 'layers/layer_fc2/activation_mean': -12.759440422058105, 'layers/layer_fc3/activation_mean': -5.810527801513672, 'layers/layer_fc1/activation_std': 39.21321487426758, 'layers/layer_fc4/activation_std': 39.21321487426758, 'layers/layer_fc5/activation_std': 39.21321487426758, 'layers/layer_fc2/activation_std': 10.599294662475586, 'layers/layer_fc3/activation_std': 7.578292369842529, 'layers/layer_fc1.weight_mean': 4.031595744891092e-05, 'layers/layer_fc1.bias_mean': -5.974042505840771e-05, 'layers/layer_fc2.weight_mean': -1.2967424481757917e-05, 'layers/layer_fc2.bias_mean': -0.00016122810484375805, 'layers/layer_fc3.weight_mean': 6.324012247205246e-06, 'layers/layer_fc3.bias_mean': 0.00010194708738708869, 'layers/layer_fc4.weight_mean': -0.00013273996592033654, 'layers/layer_fc4.bias_mean': 0.0001236710959346965, 'layers/layer_fc5.weight_mean': -5.18019738215969e-10, 'layers/layer_fc5.bias_mean': -6.053596512956005e-10, 'layers/layer_fc1.weight_std': 0.00062244210857898, 'layers/layer_fc1.bias_std': 0.0006770123727619648, 'layers/layer_fc2.weight_std': 0.0038620077539235353, 'layers/layer_fc2.bias_std': 0.0019892167765647173, 'layers/layer_fc3.weight_std': 0.003400410758331418, 'layers/layer_fc3.bias_std': 0.004144275560975075, 'layers/layer_fc4.weight_std': 0.0041240728460252285, 'layers/layer_fc4.bias_std': 0.004583150614053011, 'layers/layer_fc5.weight_std': 0.019711505621671677, 'layers/layer_fc5.bias_std': 0.02345249615609646, 'grad_norm/fc1.weight': 0.2794375717639923, 'grad_norm/fc1.bias': 0.010853193700313568, 'grad_norm/fc2.weight': 0.756790280342102, 'grad_norm/fc2.bias': 0.024361643940210342, 'grad_norm/fc3.weight': 0.4164503812789917, 'grad_norm/fc3.bias': 0.041247621178627014, 'grad_norm/fc4.weight': 0.3300709128379822, 'grad_norm/fc4.bias': 0.036391083151102066, 'grad_norm/fc5.weight': 0.49827632308006287, 'grad_norm/fc5.bias': 0.07035749405622482}\n",
      "Epoch [2/5] Training complete. Loss: 0.9144, Accuracy: 67.32%\n",
      "{'layers/layer_fc1/activation_mean': -162.39700317382812, 'layers/layer_fc4/activation_mean': -162.39700317382812, 'layers/layer_fc5/activation_mean': -162.39700317382812, 'layers/layer_fc2/activation_mean': -17.86925506591797, 'layers/layer_fc3/activation_mean': -7.695600986480713, 'layers/layer_fc1/activation_std': 39.54920959472656, 'layers/layer_fc4/activation_std': 39.54920959472656, 'layers/layer_fc5/activation_std': 39.54920959472656, 'layers/layer_fc2/activation_std': 12.200335502624512, 'layers/layer_fc3/activation_std': 7.782861709594727, 'layers/layer_fc1.weight_mean': -3.351208579260856e-05, 'layers/layer_fc1.bias_mean': 4.919838102068752e-05, 'layers/layer_fc2.weight_mean': 3.6362267564982176e-05, 'layers/layer_fc2.bias_mean': 0.00021275453036651015, 'layers/layer_fc3.weight_mean': 1.2260925359441899e-05, 'layers/layer_fc3.bias_mean': 0.0003240771184209734, 'layers/layer_fc4.weight_mean': -0.00010540984658291563, 'layers/layer_fc4.bias_mean': -0.0003542335471138358, 'layers/layer_fc5.weight_mean': 1.9608706325335845e-10, 'layers/layer_fc5.bias_mean': 1.8626451769865326e-10, 'layers/layer_fc1.weight_std': 0.0006732655456289649, 'layers/layer_fc1.bias_std': 0.0007651570485904813, 'layers/layer_fc2.weight_std': 0.007791644427925348, 'layers/layer_fc2.bias_std': 0.004374559037387371, 'layers/layer_fc3.weight_std': 0.004076257348060608, 'layers/layer_fc3.bias_std': 0.0062528871931135654, 'layers/layer_fc4.weight_std': 0.00545633165165782, 'layers/layer_fc4.bias_std': 0.006939326878637075, 'layers/layer_fc5.weight_std': 0.016751136630773544, 'layers/layer_fc5.bias_std': 0.014692811295390129, 'grad_norm/fc1.weight': 0.3019956946372986, 'grad_norm/fc1.bias': 0.012243907898664474, 'grad_norm/fc2.weight': 1.5268410444259644, 'grad_norm/fc2.bias': 0.05346183478832245, 'grad_norm/fc3.weight': 0.49922317266464233, 'grad_norm/fc3.bias': 0.062299784272909164, 'grad_norm/fc4.weight': 0.43655386567115784, 'grad_norm/fc4.bias': 0.055152054876089096, 'grad_norm/fc5.weight': 0.4234427511692047, 'grad_norm/fc5.bias': 0.044078435748815536}\n",
      "Epoch [3/5] Training complete. Loss: 0.7789, Accuracy: 73.94%\n",
      "{'layers/layer_fc1/activation_mean': -162.37669372558594, 'layers/layer_fc4/activation_mean': -162.37669372558594, 'layers/layer_fc5/activation_mean': -162.37669372558594, 'layers/layer_fc2/activation_mean': -18.648223876953125, 'layers/layer_fc3/activation_mean': -7.7249956130981445, 'layers/layer_fc1/activation_std': 39.64369201660156, 'layers/layer_fc4/activation_std': 39.64369201660156, 'layers/layer_fc5/activation_std': 39.64369201660156, 'layers/layer_fc2/activation_std': 13.20457935333252, 'layers/layer_fc3/activation_std': 8.523029327392578, 'layers/layer_fc1.weight_mean': -4.384123894851655e-05, 'layers/layer_fc1.bias_mean': 5.376951958169229e-05, 'layers/layer_fc2.weight_mean': 0.00012754423369187862, 'layers/layer_fc2.bias_mean': 0.0002990294306073338, 'layers/layer_fc3.weight_mean': 7.330457447096705e-05, 'layers/layer_fc3.bias_mean': 0.0008429083391092718, 'layers/layer_fc4.weight_mean': -0.00011236544378334656, 'layers/layer_fc4.bias_mean': -0.0007185321301221848, 'layers/layer_fc5.weight_mean': -1.0186340659856796e-10, 'layers/layer_fc5.bias_mean': 7.450580707946131e-10, 'layers/layer_fc1.weight_std': 0.000986010069027543, 'layers/layer_fc1.bias_std': 0.001086454140022397, 'layers/layer_fc2.weight_std': 0.012911902740597725, 'layers/layer_fc2.bias_std': 0.0031231597531586885, 'layers/layer_fc3.weight_std': 0.006555264815688133, 'layers/layer_fc3.bias_std': 0.006345531903207302, 'layers/layer_fc4.weight_std': 0.008043059147894382, 'layers/layer_fc4.bias_std': 0.0076463837176561356, 'layers/layer_fc5.weight_std': 0.038150131702423096, 'layers/layer_fc5.bias_std': 0.0253444854170084, 'grad_norm/fc1.weight': 0.4421679377555847, 'grad_norm/fc1.bias': 0.017370598390698433, 'grad_norm/fc2.weight': 2.5302963256835938, 'grad_norm/fc2.bias': 0.038298528641462326, 'grad_norm/fc3.weight': 0.8028761744499207, 'grad_norm/fc3.bias': 0.06369742006063461, 'grad_norm/fc4.weight': 0.6434571743011475, 'grad_norm/fc4.bias': 0.06096290051937103, 'grad_norm/fc5.weight': 0.9643760919570923, 'grad_norm/fc5.bias': 0.07603345066308975}\n",
      "Epoch [4/5] Training complete. Loss: 0.7422, Accuracy: 75.72%\n",
      "{'layers/layer_fc1/activation_mean': -162.26683044433594, 'layers/layer_fc4/activation_mean': -162.26683044433594, 'layers/layer_fc5/activation_mean': -162.26683044433594, 'layers/layer_fc2/activation_mean': -24.927921295166016, 'layers/layer_fc3/activation_mean': -10.208073616027832, 'layers/layer_fc1/activation_std': 40.17057800292969, 'layers/layer_fc4/activation_std': 40.17057800292969, 'layers/layer_fc5/activation_std': 40.17057800292969, 'layers/layer_fc2/activation_std': 16.805315017700195, 'layers/layer_fc3/activation_std': 9.891535758972168, 'layers/layer_fc1.weight_mean': -5.1961251301690936e-05, 'layers/layer_fc1.bias_mean': 6.783485878258944e-05, 'layers/layer_fc2.weight_mean': 0.00011843817628687248, 'layers/layer_fc2.bias_mean': 0.0007921061478555202, 'layers/layer_fc3.weight_mean': 0.0001514973264420405, 'layers/layer_fc3.bias_mean': 0.0010678424732759595, 'layers/layer_fc4.weight_mean': 1.0874953659367748e-05, 'layers/layer_fc4.bias_mean': -4.5362801756709814e-05, 'layers/layer_fc5.weight_mean': 1.8471837948119685e-10, 'layers/layer_fc5.bias_mean': 5.587935669737476e-10, 'layers/layer_fc1.weight_std': 0.0006585291703231633, 'layers/layer_fc1.bias_std': 0.0007209584582597017, 'layers/layer_fc2.weight_std': 0.010477228090167046, 'layers/layer_fc2.bias_std': 0.005132471676915884, 'layers/layer_fc3.weight_std': 0.008328537456691265, 'layers/layer_fc3.bias_std': 0.008772918954491615, 'layers/layer_fc4.weight_std': 0.005717130843549967, 'layers/layer_fc4.bias_std': 0.005424214527010918, 'layers/layer_fc5.weight_std': 0.031180905178189278, 'layers/layer_fc5.bias_std': 0.030189601704478264, 'grad_norm/fc1.weight': 0.29593732953071594, 'grad_norm/fc1.bias': 0.011563831008970737, 'grad_norm/fc2.weight': 2.053213596343994, 'grad_norm/fc2.bias': 0.06339646875858307, 'grad_norm/fc3.weight': 1.0201679468154907, 'grad_norm/fc3.bias': 0.08794017881155014, 'grad_norm/fc4.weight': 0.45733556151390076, 'grad_norm/fc4.bias': 0.04305489733815193, 'grad_norm/fc5.weight': 0.7882049083709717, 'grad_norm/fc5.bias': 0.09056880325078964}\n",
      "Epoch [5/5] Training complete. Loss: 0.7054, Accuracy: 76.78%\n",
      "{'layers/layer_fc1/activation_mean': -162.22689819335938, 'layers/layer_fc4/activation_mean': -162.22689819335938, 'layers/layer_fc5/activation_mean': -162.22689819335938, 'layers/layer_fc2/activation_mean': -26.883211135864258, 'layers/layer_fc3/activation_mean': -14.033665657043457, 'layers/layer_fc1/activation_std': 40.35655975341797, 'layers/layer_fc4/activation_std': 40.35655975341797, 'layers/layer_fc5/activation_std': 40.35655975341797, 'layers/layer_fc2/activation_std': 18.789628982543945, 'layers/layer_fc3/activation_std': 13.676512718200684, 'layers/layer_fc1.weight_mean': 1.5473866369575262e-05, 'layers/layer_fc1.bias_mean': -1.7887834474095143e-05, 'layers/layer_fc2.weight_mean': -0.00011709203681675717, 'layers/layer_fc2.bias_mean': -0.00017746593221090734, 'layers/layer_fc3.weight_mean': -9.6314717666246e-05, 'layers/layer_fc3.bias_mean': -0.0003019513678736985, 'layers/layer_fc4.weight_mean': 7.599956006743014e-05, 'layers/layer_fc4.bias_mean': 7.583803380839527e-06, 'layers/layer_fc5.weight_mean': 1.14232533154901e-10, 'layers/layer_fc5.bias_mean': 0.0, 'layers/layer_fc1.weight_std': 0.0006393238436430693, 'layers/layer_fc1.bias_std': 0.0007226161542348564, 'layers/layer_fc2.weight_std': 0.01606857031583786, 'layers/layer_fc2.bias_std': 0.002762269927188754, 'layers/layer_fc3.weight_std': 0.009842080064117908, 'layers/layer_fc3.bias_std': 0.0029760582838207483, 'layers/layer_fc4.weight_std': 0.008030076511204243, 'layers/layer_fc4.bias_std': 0.005211194045841694, 'layers/layer_fc5.weight_std': 0.02980860136449337, 'layers/layer_fc5.bias_std': 0.024154536426067352, 'grad_norm/fc1.weight': 0.2865002751350403, 'grad_norm/fc1.bias': 0.01154280360788107, 'grad_norm/fc2.weight': 3.1488263607025146, 'grad_norm/fc2.bias': 0.03378778323531151, 'grad_norm/fc3.weight': 1.2054210901260376, 'grad_norm/fc3.bias': 0.02976495958864689, 'grad_norm/fc4.weight': 0.6423847079277039, 'grad_norm/fc4.bias': 0.041362613439559937, 'grad_norm/fc5.weight': 0.7535152435302734, 'grad_norm/fc5.bias': 0.07246360927820206}\n",
      "Testing complete. Loss: 0.6699, Accuracy: 78.28%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "activation_dict_mean = {}\n",
    "activation_dict_std = {}\n",
    "params_dict_std = {}\n",
    "params_dict_mean = {}\n",
    "grad_norms = {}\n",
    "\n",
    "num_epochs = params[\"epochs\"]\n",
    "step_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    # Reset activations for each epoch\n",
    "    model.clear_activations()\n",
    "    \n",
    "    # Training step\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        step_counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_preds += target.size(0)\n",
    "        correct_preds += (predicted == target).sum().item()\n",
    "        \n",
    "        # Print loss and accuracy for each batch (step)\n",
    "        #if (batch_idx + 1) % 5 == 0:  # Every 5 steps\n",
    "        batch_accuracy = 100 * correct_preds / total_preds\n",
    "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.2f}%\")\n",
    "                \n",
    "        # Validation step per training step\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader)  # Evaluate after each step\n",
    "        # print(f\"Validation at step [{batch_idx+1}/{len(train_loader)}] - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        run.log_metrics(\n",
    "            data = {\n",
    "                \"metrics/train/loss\": loss.item(),\n",
    "                \"metrics/train/accuracy\": batch_accuracy,\n",
    "                \"metrics/validation/loss\": val_loss,\n",
    "                \"metrics/validation/accuracy\": val_accuracy,\n",
    "                \"epoch_value\": epoch\n",
    "            },\n",
    "            step = step_counter\n",
    "        )\n",
    "    \n",
    "    # Print loss and accuracy for the entire training epoch\n",
    "    train_accuracy = 100 * correct_preds / total_preds\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training complete. Loss: {epoch_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Track activations\n",
    "    for name, activation in model.get_activations().items():\n",
    "        activation_dict_mean[f\"layers/layer_{name}/activation_mean\"] = activation.mean().item()\n",
    "        activation_dict_std[f\"layers/layer_{name}/activation_std\"] = activation.std().item()\n",
    "\n",
    "    # Track gradients and norms per layer at each epoch\n",
    "    for name, param in model.named_parameters():\n",
    "        if param is not None:\n",
    "            params_dict_std[f\"layers/layer_{name}_std\"] = param.grad.std().item()\n",
    "            params_dict_mean[f\"layers/layer_{name}_mean\"] = param.grad.mean().item()\n",
    "            grad_norms[f\"grad_norm/{name}\"] = param.grad.norm(2).item() # L2 norm (Euclidean norm) of the gradients\n",
    "    \n",
    "    layers_dict = {**activation_dict_mean, \n",
    "                   **activation_dict_std,\n",
    "                   **params_dict_mean,\n",
    "                   **params_dict_std,\n",
    "                   **grad_norms\n",
    "                   }\n",
    "    print(layers_dict)\n",
    "\n",
    "    # data_to_log = {\n",
    "    #        \"metrics/test/loss_epoch\": epoch_loss / len(train_loader),\n",
    "     #       \"metrics/train/accuracy_epoch\": train_accuracy\n",
    "      #  }.update(activation_dict)\n",
    "    \n",
    "    run.log_metrics(\n",
    "    data = layers_dict,\n",
    "    step = epoch\n",
    "    )\n",
    "    \n",
    "# Final Testing Step with gradient tracking\n",
    "test_loss, test_accuracy = evaluate(model, test_loader, track_gradients=False)  # Track gradients during test\n",
    "print(f\"Testing complete. Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "run.log_configs(\n",
    "        {\n",
    "        \"metrics/test/loss\": test_loss,\n",
    "        \"metrics/test/accuracy\": test_accuracy\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neptune:INFO: Waiting for all operations to be processed\n",
      "neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "run.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
