{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune + PyTorch\n",
    "\n",
    "## Logging and Visualizing debugging metrics with Neptune\n",
    "\n",
    "### Introduction\n",
    "\n",
    "See how Neptune Scale can be used for pre-training models like foundation models by tracking hundreds of metrics. This example is designed to be used as a code recipe for you to re-use sections with your own code to edit or adapt to your own model training needs. \n",
    "\n",
    "This guide will show you how to:\n",
    "- Initialize the Neptune Run object and log configuration parameters\n",
    "- Log standard loss and accuracy metrics to Neptune\n",
    "- Log _**debugging metrics**_ per layer during model training such as;\n",
    "    * Activations\n",
    "    * Gradients\n",
    "    * Parameters (Weights and Biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "  1. Create a Neptune Scale account. [Register &rarr;](https://neptune.ai/early-access)\n",
    "  2. Create a Neptune project that you will use for tracking metadata. For instructions, see [Projects](https://docs-beta.neptune.ai/projects/) in the Neptune Scale docs.\n",
    "  3. Install and configure Neptune Scale for logging metadata. For instructions, see [Get started](https://docs-beta.neptune.ai/setup) in the Neptune Scale docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q -U neptune_scale torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neptune_scale import Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "\n",
    "params = {\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"epochs\": 5, \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"input_features\": 256,\n",
    "    \"embed_size\": 1000,\n",
    "    \"hidden_size\": 256, # hidden size for the LSTM\n",
    "    \"dropout_prob\": 0.3,\n",
    "    \"num_lstm_layers\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download or use next token prediction dataset\n",
    "The dataset used in this example is taken from [HuggingFace](https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset). In this example, you can increase the size of the dataset to test the logging capabilities of Neptune, but note that increasing the dataset size will increase the time taken for the full dataset to download. The current setup only downloads the first parquet file from the Hugging Face public dataset. The validation dataset is also reduced to reduce the training loop execution time - you can increase the validation size by changing the `test_size` key-value pair in the `train_test_split()` method from HF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 81926 \n",
      "Validation samples: 935\n"
     ]
    }
   ],
   "source": [
    "# For the example, download a random subset of 10% of the original dataset\n",
    "base_url = \"https://huggingface.co/datasets/Na0s/Next_Token_Prediction_dataset/resolve/main/data/\"\n",
    "data_files = {\"train\": base_url + \"train-00001-of-00067.parquet\", # download only the first 10 files from the HF dataset\n",
    "              \"validation\": base_url + \"validation-00000-of-00001.parquet\"} #doanload the complete validation dataset\n",
    "\n",
    "data_subset = load_dataset(\"parquet\", data_files = data_files, num_proc=4)\n",
    "# validation_subset = load_dataset(\"parquet\", data_files = {\"validation\": base_url + \"validation-00000-of-00001.parquet\"}, num_proc=4, split=[\"validation[:5%]\"])\n",
    "validation_subset = data_subset.get(\"validation\").train_test_split(test_size=0.1)\n",
    "print(f\"Training samples: {data_subset[\"train\"].num_rows} \\nValidation samples: {validation_subset[\"test\"].num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader Objects\n",
    "* To execute the models with PyTorch, we convert the training and validation datasets to tensors and then setup DataLoader for easier batching in our training loop.\n",
    "* The model architecture requires the vocabulary size as an input and this we calcualte the max token from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 128257\n"
     ]
    }
   ],
   "source": [
    "train_subset = data_subset[\"train\"].with_format(type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]) # HF provides methods to convert datatypes to tensors\n",
    "validation_subset = validation_subset[\"test\"].with_format(type=\"torch\", columns=[\"text\", \"input_ids\", \"labels\"]) # HF provides methods to convert datatypes to tensors\n",
    "\n",
    "train_dataloader = DataLoader(train_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(validation_subset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# Determine the vocab size of the dataset\n",
    "# Flatten the list of tokenized sentences into one long list of token IDs\n",
    "vocab_size = max([token for sentence in data_subset[\"train\"][\"input_ids\"] for token in sentence]) + 1\n",
    "params[\"vocab_size\"] = vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Logging Debugging Metrics\n",
    "\n",
    "During model training, capturing valuable information from each layer can offer critical insights into the model's learning process, identify potential issues, and guide improvements. Monitoring certain metrics and activations helps diagnose common problems such as vanishing gradients, overfitting, or underfitting. Below are key metrics to track from each layer:\n",
    "\n",
    "### Key metrics to capture from each layer:\n",
    "\n",
    "| **Metric**                        | **Demonstrated in Notebook** | **What it Shows**                                                                                             | **How to Capture**                                             |\n",
    "|-----------------------------------|--------------------------------------|--------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Activations**                   | Yes                                  | Provides insight into how the model is processing data. Dead or exploding activations can indicate issues with training stability. | Use hooks to capture activations after each layer.            |\n",
    "| **Gradients**                     | Yes                                  | Essential for diagnosing vanishing or exploding gradients. Small gradients may indicate vanishing gradients, while large ones can signal instability. | Use hooks to capture gradients during backpropagation.        |\n",
    "| **Weights and Biases**            | Yes                                  | Tracks how the model’s parameters evolve during training. Large or small weights may indicate the need for better regularization or adjustments in learning rate. | Extract directly from the model’s parameters.                  |\n",
    "| **Layer-wise Loss**               | No                                   | Identifies which parts of the network contribute more to the overall loss, aiding debugging and optimization. | Monitor outputs from each layer and compare with the target.   |\n",
    "| **Learning Rate per Layer**       | No                                   | Helpful if using techniques like Layer-wise Learning Rate Decay (L2LRD). Tracking this can provide insight into the layer-specific learning rate. | Manually track based on optimizer settings.                    |\n",
    "| **Layer Output Norms**            | No                                   | The L2-norm of layer outputs can highlight issues like gradient explosion or vanishing gradients. | Compute the L2-norm for each layer’s output.                   |\n",
    "| **Activation Distributions**      | No                                   | Helps diagnose saturation issues, especially with ReLU activations that may lead to dead neurons. | Visualize or compute statistical summaries using tools like matplotlib or seaborn. |\n",
    "| **Feature Maps (for Convolutional Layers)** | No                                   | Offers insights into how convolutional layers detect specific patterns in the data. | Visualize feature maps after convolutional layers using libraries like matplotlib. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTorch Model Architecture and Helpers\n",
    "We define a simple LLM model architecture using PyTorch. Since this is a text-based example, we use an embedding layer, a LSTM layer and a fully connected layer. This architecture can be adjusted to your needs and increased in size when testing the workflow. To increase the size of the LSTM layers, change the `num_layers` parameter in the parameters dictionary or to increase the number of fully connected layers, update the mode architecture itself.\n",
    "\n",
    "This section also creates a `HookManager` class that allows us to capture the **activations** and **gradients** from each layer. You do not need to update this class as it will dynamically update according to the architecture of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple LLM model with LSTM\n",
    "class SimpleLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(SimpleLLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers = num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM returns output and hidden/cell state tuple\n",
    "        out = self.fc1(lstm_out) # Use the last output from the LSTM\n",
    "        return out\n",
    "\n",
    "# A class to manage hooks for activations and gradients\n",
    "class HookManager:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.hooks = []\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    # Function to save activations\n",
    "    def save_activation(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.activations[name] = output\n",
    "        return hook\n",
    "\n",
    "    # Function to save gradients (registering hooks for the model parameters)\n",
    "    def save_gradient(self, name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            self.gradients[name] = grad_output[0]\n",
    "        return hook\n",
    "\n",
    "    # Function to register hooks for activations and gradients\n",
    "    def register_hooks(self):\n",
    "        # Register forward hooks for activations\n",
    "        for name, module in self.model.named_modules():\n",
    "            self.hooks.append(module.register_forward_hook(self.save_activation(name)))\n",
    "\n",
    "        # Register backward hooks for gradients\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, (nn.LSTM, nn.Linear)): # You can add more layer types here\n",
    "                self.hooks.append(module.register_full_backward_hook(self.save_gradient(name)))\n",
    "\n",
    "    # Function to clear activations and gradients after use\n",
    "    def clear(self):\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    # Function to get activations\n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n",
    "    # Function to get gradients\n",
    "    def get_gradients(self):\n",
    "        return self.gradients\n",
    "\n",
    "# Function to evaluate the model after each epoch/step\n",
    "def evaluate(model, val_dataloader, criterion, device, vocab_size):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            logits = model(input_ids)  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_dataloader)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Model Training\n",
    "### Initialize Neptune Run object and Log Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neptune parameters\n",
    "from neptune_scale import Run\n",
    "from uuid import uuid4\n",
    "\n",
    "run = Run(\n",
    "    project = \"examples/pytorch-tutorial\",\n",
    "    run_id=f\"pytorch-text-{uuid4()}\"\n",
    "    )\n",
    "\n",
    "run.log_configs(\n",
    "    {\n",
    "        \"config/learning_rate\": params[\"learning_rate\"],\n",
    "        \"config/optimizer\": params[\"optimizer\"],\n",
    "        \"config/batch_size\": params[\"batch_size\"],\n",
    "        \"config/epochs\": params[\"epochs\"],\n",
    "        \"config/num_lstm_layers\" : params[\"num_lstm_layers\"],\n",
    "        \"data/vocab_size\": params[\"vocab_size\"],\n",
    "        \"data/embed_size\": params[\"embed_size\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "run.add_tags(tags=[params[\"optimizer\"]], group_tags=True)\n",
    "run.add_tags(tags=[\"text\", \"LLM\", \"Simple\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute model training loop\n",
    "In this loop, we configure the `HookManager` and register the hooks. In your training loop, you will need to use the `get_` methods to retrieve the stored values for the activations and gradients after the forward and backward passes are complete. Below, you can see a pseudo implementation:\n",
    "\n",
    "```python\n",
    "# Initialize model\n",
    "model = your_ModelClass()\n",
    "# Register hooks\n",
    "hm = HookManager(model)\n",
    "hm.register_hooks()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    \n",
    "    # Forward pass, e.g. model.train()\n",
    "    # Backward pass, e.g. loss.backward()\n",
    "    \n",
    "    activations = hm.get_activations()\n",
    "    gradients = hm.get_gradients()\n",
    "\n",
    "    # Log values (mean, std, etc.) to Neptune\n",
    "```\n",
    "\n",
    "Important: The `HookManager` class can be used in your own training script as it only accepts a model object as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 / 10241, Loss: 11.760001182556152\n",
      "Step 2 / 10241, Loss: 11.625699996948242\n",
      "Step 3 / 10241, Loss: 9.880274772644043\n",
      "Step 4 / 10241, Loss: 9.80041790008545\n",
      "Step 5 / 10241, Loss: 10.201050758361816\n",
      "Step 5, Val_loss: 9.920373557979225\n",
      "Gradients for fc1: -3.875334975882508e-15\n",
      "Gradients for lstm: -2.1588371055258904e-06\n",
      "Step 6 / 10241, Loss: 9.584494590759277\n",
      "Step 7 / 10241, Loss: 9.479751586914062\n",
      "Step 8 / 10241, Loss: 10.132659912109375\n",
      "Step 9 / 10241, Loss: 10.202069282531738\n",
      "Step 10 / 10241, Loss: 11.208635330200195\n",
      "Step 10, Val_loss: 10.601182611579569\n",
      "Gradients for fc1: 4.1997570650103774e-15\n",
      "Gradients for lstm: -2.5467079467489384e-06\n",
      "Step 11 / 10241, Loss: 10.122359275817871\n",
      "Step 12 / 10241, Loss: 10.62948226928711\n",
      "Step 13 / 10241, Loss: 10.538959503173828\n",
      "Step 14 / 10241, Loss: 11.84500503540039\n",
      "Step 15 / 10241, Loss: 10.522750854492188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Do not need to log validation at every step, although we can\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Track activations\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 69\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, val_dataloader, criterion, device, vocab_size)\u001b[0m\n\u001b[0;32m     66\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Forward pass for validation\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size, seq_len, vocab_size)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[0;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m, in \u001b[0;36mSimpleLLM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     11\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)  \u001b[38;5;66;03m# LSTM returns output and hidden/cell state tuple\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_out\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use the last output from the LSTM\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1798\u001b[0m     ):\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leo.breedt\\miniconda3\\envs\\neptune_scale_py_312_base\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "debug_metrics = {}\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = SimpleLLM(params[\"vocab_size\"], params[\"embed_size\"], params[\"hidden_size\"], params[\"num_lstm_layers\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr = params[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100) # Ignore the buffering index of -100 in the dataset\n",
    "\n",
    "hook_manager = HookManager(model)\n",
    "hook_manager.register_hooks()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "step_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(params[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        model.train()\n",
    "        step_counter += 1\n",
    "        hook_manager.clear()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Compute the loss (ignore padding tokens by masking labels)\n",
    "        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        print(f\"Step {step_counter} / {len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        if step_counter % 5 == 0: # Do not need to log validation at every step, although we can\n",
    "            val_loss = evaluate(model, val_dataloader, criterion, device, vocab_size)\n",
    "            print(f\"Step {step_counter}, Val_loss: {val_loss}\")\n",
    "\n",
    "            # Track activations\n",
    "            activations = hook_manager.get_activations()\n",
    "            for layer, activation in activations.items():\n",
    "                if layer is not None:\n",
    "                    debug_metrics[f\"debug/activation/{layer}_mean\"] = activation[0].mean().item()\n",
    "                    debug_metrics[f\"debug/activation/{layer}_std\"] = activation[0].std().item()\n",
    "\n",
    "            # Track gradients with hooks\n",
    "            gradients = hook_manager.get_gradients()\n",
    "            for layer, gradient in gradients.items():\n",
    "               debug_metrics[f\"debug/gradient/{layer}_mean\"] = gradient.mean().item()\n",
    "               print(f\"Gradients for {layer}: {gradient.mean().item()}\") # You can replace to use mean(), sum(), max() or min()\n",
    "               # simplified_gradient = gradient.mean(dim=(0, 1))\n",
    "               # print(f\"Summed Gradient for {layer}: {simplified_gradient}\")\n",
    "\n",
    "            # Track gradients per layer at each epoch\n",
    "            for layer, param in model.named_parameters():\n",
    "                if param is not None:\n",
    "                    debug_metrics[f\"debug/parameters/{layer}_std\"] = param.grad.std().item()\n",
    "                    debug_metrics[f\"debug/parameters/{layer}_mean\"] = param.grad.mean().item()\n",
    "                    debug_metrics[f\"debug/parameters/{layer}_norm\"] = param.grad.norm().item() # L2 norm (Euclidean norm) of the gradients\n",
    "\n",
    "            # Output loss for this epoch\n",
    "            run.log_metrics(\n",
    "                data = {\n",
    "                    \"metrics/train/loss\": loss.item(),\n",
    "                    \"metrics/validation/loss\": val_loss,\n",
    "                    \"epoch/value\": epoch,\n",
    "                    **debug_metrics\n",
    "                },\n",
    "                step = step_counter\n",
    "            )\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "# test_loss = evaluate_model(model, test_input, test_target, params[\"vocab_size\"])\n",
    "# print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neptune:INFO: Waiting for all operations to be processed\n",
      "neptune:WARNING: No timeout specified. Waiting indefinitely\n",
      "neptune:INFO: All operations were processed\n"
     ]
    }
   ],
   "source": [
    "# Close run to ensure all operations are processed\n",
    "run.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neptune_scale_py_312_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
